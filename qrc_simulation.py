#!/usr/bin/env python3
"""
Quantum Reservoir Computing (QRC) Simulation
Refactored from Tutorial.ipynb for easier debugging
"""

import matplotlib.pyplot as plt
import torch as pt
import numpy as np
import pandas as pd
import scipy
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler
from torch.utils.data import DataLoader, TensorDataset
import time
import pickle

from tqdm import tqdm
import os
import json

# Import utilities
from utilities import *


class QRCSimulator:
    """Quantum Reservoir Computing Simulator"""
    
    def __init__(self, total_spins=6, input_sys_size=1, T_evolution=10):
        """
        Initialize QRC simulator
        
        Args:
            total_spins: Total number of quantum spins
            input_sys_size: Size of input system 
            T_evolution: Time evolution parameter
        """
        self.total_spins = total_spins
        self.input_sys_size = input_sys_size
        self.T_evolution = T_evolution
        
        # Initialize Hilbert spaces
        self._setup_hilbert_spaces()
        
        # Initialize quantum state
        self._setup_initial_state()
        
        # Setup basis change matrices
        self._setup_basis_changes()
        
        # Setup data structure
        self._setup_data_structure()
        
    def _setup_hilbert_spaces(self):
        """Setup Hilbert spaces for the quantum system"""
        self.spin_space_B = HilbertSpace(self.total_spins - self.input_sys_size, 'spin')
        self.spin_space_A = HilbertSpace(self.input_sys_size, 'spin')
        
        hilbert_spaces_dict = {
            'spin_A': self.spin_space_A,
            'spin_B': self.spin_space_B
        }
        self.composite_space = CompositeHilbertSpace(hilbert_spaces_dict)
        
    def _setup_initial_state(self):
        """Setup initial quantum state |000000>"""
        rho_0_spins = np.zeros((2**self.total_spins, 2**self.total_spins), dtype=np.complex128)
        rho_0_spins[0][0] = 1.0 + 1j*0.0
        self.rho_0 = DensityMatrix(rho_0_spins)
        
    def _setup_basis_changes(self):
        """Setup basis change matrices for X, Y measurements"""
        # Hadamard matrix for X basis
        H = 1/np.sqrt(2) * np.array([[1, 1], [1, -1]], dtype=complex)
        H_total = H
        for i in range(self.total_spins - 1):
            H_total = np.kron(H_total, H)
        self.X_basis_change = H_total
        self.X_basis_change_dag = H_total.conj().T
        
        # Phase gate for Y basis
        S = np.array([[1, 0], [0, -np.exp(1j*np.pi/2)]], dtype=complex)
        S_total = S
        for i in range(self.total_spins - 1):
            S_total = np.kron(S_total, S)
        self.Y_basis_change = H_total @ S_total
        self.Y_basis_change_dag = self.Y_basis_change.conj().T
        
    def _setup_data_structure(self):
        """Setup data structure for storing results"""
        column_length_dict = {
            'X': self.total_spins,
            'Y': self.total_spins,
            'Z': self.total_spins,
            'XX': self.total_spins * (self.total_spins + 1) // 2,
            'YY': self.total_spins * (self.total_spins + 1) // 2,
            'ZZ': self.total_spins * (self.total_spins + 1) // 2
        }
        
        self.df_columns = []
        for col in column_length_dict.keys():
            for i in range(column_length_dict[col]):
                self.df_columns.append(col + str(i))
    
    @staticmethod
    def get_couplings(n_spins, J):
        """Generate random coupling matrix"""
        J_bare = np.random.uniform(low=-0.5+J, high=0.5*J, size=(n_spins, n_spins))
        # Exclude self-interaction
        for i in range(n_spins):
            J_bare[i, i] = 0
        # Make symmetric
        J_bare = np.triu(J_bare, k=1)
        J_bare = 0.5 * (J_bare + J_bare.T)
        return J_bare
    
    def get_hamiltonian(self, h=10, J=1):
        """
        Generate reservoir Hamiltonian
        
        Args:
            h: Magnetic field strength
            J: Coupling strength
            
        Returns:
            H_reservoir: Reservoir Hamiltonian matrix
        """
        H_reservoir = np.zeros((self.composite_space.dimension, self.composite_space.dimension))
        J_bare = self.get_couplings(self.total_spins, J)
        
        # XX coupling terms
        for i in range(1, self.total_spins + 1):
            for j in range(i + 1, self.total_spins + 1):
                H_reservoir += J * J_bare[i-1, j-1] * self.composite_space.X[i] @ self.composite_space.X[j]
        
        # Z field terms
        for i in range(1, self.total_spins + 1):
            H_reservoir += 0.5 * h * self.composite_space.Z[i]
            
        return H_reservoir
    
    @staticmethod
    def M_matrix(N, g):
        """Generate measurement backaction matrix"""
        Mi = np.array([[1.0, np.exp(-g**2/2)], [np.exp(-g**2/2), 1]], dtype=np.complex128)
        M = Mi * 1
        for j in range(N - 1):
            M = np.kron(M, Mi)
        return M
    
    def run_simulation(self, time_series, g=0.5, h=1, verbose=True):
        """
        Run QRC simulation for given parameters
        
        Args:
            time_series: Input time series data
            g: Measurement strength parameter
            h: Magnetic field strength
            verbose: Whether to print progress
            
        Returns:
            features_df: DataFrame with extracted features
        """
        if verbose:
            print(f"Running QRC simulation with g={g}, h={h}")
            
        # Initialize Hamiltonian and evolution
        H_reservoir = self.get_hamiltonian(h)
        U_reservoir = scipy.linalg.expm(-1j * H_reservoir * self.T_evolution)
        U_reservoir_dag = U_reservoir.conj().T
        
        # Measurement backaction matrix
        M = self.M_matrix(self.total_spins, g)
        
        # Initialize data storage
        data = {
            'X': [], 'Y': [], 'Z': [],
            'XX': [], 'YY': [], 'ZZ': []
        }
        
        K = len(time_series)
        
        # Process time series for each observable
        for obs in ['X', 'Y', 'Z']:
            if verbose:
                print(f"Processing {obs} observable...")
                
            rho = self.rho_0.dm
            
            for k in tqdm(range(K), disable=not verbose):
                if isinstance(rho, DensityMatrix):
                    rho = rho.dm
                
                # Input encoding
                s_k = time_series[k]
                rho_B = self.composite_space.get_reduced_density_matrix(rho, ['spin_B'])
                rho_A = np.array([
                    [1. - s_k, np.sqrt((1 - s_k) * s_k)],
                    [np.sqrt((1 - s_k) * s_k), s_k]
                ])
                rho = np.kron(rho_A, rho_B)
                
                # Time evolution
                rho = U_reservoir @ rho @ U_reservoir_dag
                
                # Measurement for different observables
                if obs == 'Z':
                    rho = self._measure_Z(rho, M, data)
                elif obs == 'Y':
                    rho = self._measure_Y(rho, M, data)
                elif obs == 'X':
                    rho = self._measure_X(rho, M, data)
        
        # Convert to DataFrame
        data_arr = self._dict_to_arr(data)
        features_df = pd.DataFrame(data_arr, columns=self.df_columns)
        
        if verbose:
            print(f"Simulation completed. Features shape: {features_df.shape}")
            
        return features_df
    
    def _measure_Z(self, rho, M, data):
        """Measure Z observables"""
        # Apply measurement
        rho = np.multiply(M, rho)
        rho = DensityMatrix(rho)
        
        # Single-spin Z expectation values
        ev_Z = []
        for i in range(1, self.total_spins + 1):
            ev_Z.append(rho.get_expectation_value(self.composite_space.Z[i]))
        data['Z'].append(ev_Z)
        
        # Two-spin ZZ correlations
        ev_ZZ = []
        for i in range(1, self.total_spins + 1):
            for j in range(i, self.total_spins + 1):
                ev_ZZ.append(rho.get_expectation_value(
                    self.composite_space.Z[i] @ self.composite_space.Z[j]
                ))
        data['ZZ'].append(ev_ZZ)
        
        return rho.dm
    
    def _measure_Y(self, rho, M, data):
        """Measure Y observables"""
        # Rotate to Y basis
        rho = self.Y_basis_change @ rho @ self.Y_basis_change_dag
        # Apply measurement
        rho = np.multiply(M, rho)
        # Rotate back to Z basis
        rho = self.Y_basis_change_dag @ rho @ self.Y_basis_change
        rho = DensityMatrix(rho)
        
        # Single-spin Y expectation values
        ev_Y = []
        for i in range(1, self.total_spins + 1):
            ev_Y.append(rho.get_expectation_value(self.composite_space.Y[i]))
        data['Y'].append(ev_Y)
        
        # Two-spin YY correlations
        ev_YY = []
        for i in range(1, self.total_spins + 1):
            for j in range(i, self.total_spins + 1):
                ev_YY.append(rho.get_expectation_value(
                    self.composite_space.Y[i] @ self.composite_space.Y[j]
                ))
        data['YY'].append(ev_YY)
        
        return rho.dm
    
    def _measure_X(self, rho, M, data):
        """Measure X observables"""
        # Rotate to X basis
        rho = self.X_basis_change @ rho @ self.X_basis_change_dag
        # Apply measurement
        rho = np.multiply(M, rho)
        # Rotate back to Z basis
        rho = self.X_basis_change_dag @ rho @ self.X_basis_change
        rho = DensityMatrix(rho)
        
        # Single-spin X expectation values
        ev_X = []
        for i in range(1, self.total_spins + 1):
            ev_X.append(rho.get_expectation_value(self.composite_space.X[i]))
        data['X'].append(ev_X)
        
        # Two-spin XX correlations
        ev_XX = []
        for i in range(1, self.total_spins + 1):
            for j in range(i, self.total_spins + 1):
                ev_XX.append(rho.get_expectation_value(
                    self.composite_space.X[i] @ self.composite_space.X[j]
                ))
        data['XX'].append(ev_XX)
        
        return rho.dm
    
    @staticmethod
    def _dict_to_arr(data_dict):
        """Convert data dictionary to structured array"""
        num_lists_per_key = len(data_dict[next(iter(data_dict))])
        concatenated_lists = []
        
        for i in range(num_lists_per_key):
            current_row = []
            for key in data_dict:
                current_row.extend(data_dict[key][i])
            concatenated_lists.append(current_row)
            
        return np.array(concatenated_lists)


class QRCAnalyzer:
    """QRC Performance Analysis Tools"""
    
    @staticmethod
    def compute_capacity(features_df, time_series, eta, f_p=True):
        """
        Compute memory/prediction capacity for given eta
        
        Args:
            features_df: Features DataFrame
            time_series: Input time series
            eta: Time delay parameter
            f_p: True for prediction, False for memory
            
        Returns:
            capacity_train, capacity_test: Training and test capacities
        """
        tot_width = features_df.shape[1]
        N_raw_data = len(features_df)
        N_skip = 20
        data_size = N_raw_data - N_skip
        
        # Prepare data tensors
        X_data = pt.zeros(data_size, tot_width, dtype=float)
        Y_ini_data = pt.zeros(data_size, 1, dtype=float)
        
        for k in range(data_size):
            X_data[k] = pt.tensor(features_df.loc[k, :].values, dtype=float)
            Y_ini_data[k] = time_series[k]
        
        eta = abs(eta)
        if f_p:
            eta = -eta
        
        Y_data = pt.roll(Y_ini_data, -eta)
        
        # Train-test split
        fraction_train = 0.7
        N_train = int(fraction_train * data_size)
        
        X_train = X_data[:N_train, :].float()
        y_train = Y_data[:N_train].float()
        X_test = X_data[-data_size + N_train:, :].float()
        y_test = Y_data[-data_size + N_train:].float()
        
        # Linear regression
        model = LinearRegression()
        model.fit(X_train, y_train)
        
        y_train_pred = model.predict(X_train)
        y_test_pred = model.predict(X_test)
        
        y_train = y_train.detach().numpy()
        y_test = y_test.detach().numpy()
        
        # Compute capacities
        capacity_train = np.cov(y_train.T, y_train_pred.T, ddof=1)[0, 1]**2 / (
            np.var(y_train, ddof=1) * np.var(y_train_pred, ddof=1)
        )
        
        capacity_test = np.cov(y_test.T, y_test_pred.T, ddof=1)[0, 1]**2 / (
            np.var(y_test, ddof=1) * np.var(y_test_pred, ddof=1)
        )
        
        return capacity_train, capacity_test
    
    @staticmethod
    def compute_sum_capacity(features_df, time_series, eta_max, f_p=True):
        """
        Compute sum capacity up to eta_max
        
        Args:
            features_df: Features DataFrame
            time_series: Input time series
            eta_max: Maximum eta value
            f_p: True for prediction, False for memory
            
        Returns:
            sum_capacity_test: Sum of test capacities
        """
        capacities_test = []
        
        for eta in range(1, eta_max + 1):
            _, capacity_test = QRCAnalyzer.compute_capacity(
                features_df, time_series, eta, f_p
            )
            capacities_test.append(capacity_test)
        
        return np.sum(capacities_test)


def load_time_series(data_type="santa_fe"):
    """
    Load and preprocess time series data
    
    Args:
        data_type: "santa_fe" or "smt"
        
    Returns:
        time_series: Preprocessed time series
    """
    if data_type == "santa_fe":
        # Forward prediction task
        time_series_raw = np.load("./sk_Santa_Fe_2000.npy")
        min_ts = min(time_series_raw)
        max_ts = max(time_series_raw)
        time_series = (time_series_raw + np.abs(min_ts)) / (max_ts - min_ts)
        time_series = time_series.flatten()
    elif data_type == "smt":
        # Memory retrieval task
        time_series = np.load("./time_series_smt.npy")

    elif data_type == "stock":
        # è¯»å–æ•°æ®
        data = pd.read_csv('./stock_price/traindata_stock.csv')['Open'].values.reshape(-1, 1)
        # data = pd.read_csv('./stock_price/testdata_stock.csv')['Open'].values.reshape(-1, 1)
        # åˆå§‹åŒ–å½’ä¸€åŒ–å™¨
        train_scaler = MinMaxScaler(feature_range=(0, 1))

        # å¯¹è®­ç»ƒæ•°æ®è¿›è¡Œæ‹Ÿåˆå’Œè½¬æ¢
        train_data_scaled = train_scaler.fit_transform(data)

        # å°†å½’ä¸€åŒ–åçš„æ•°æ®è½¬æ¢å›ä¸€ç»´æ•°ç»„
        time_series = train_data_scaled.flatten().tolist()
    else:
        raise ValueError("data_type must be 'santa_fe' or 'smt'")    
    
    return time_series


def load_figure_params():
    """Load figure parameters from JSON file"""
    with open('./figure_params.json') as json_file:
        return json.load(json_file)    


def main():
    """Main function for demonstration"""
    print("QRC Simulation Demo")
    print("==================")
    
    # Load time series
    time_series = load_time_series("santa_fe")
    print(f"Loaded time series with {len(time_series)} points")    
    
    # Initialize simulator
    simulator = QRCSimulator()
    
    # Run simulation with default parameters
    g, h = 0.5, 1
    features_df = simulator.run_simulation(time_series, g=g, h=h)
    
    # Analyze performance
    analyzer = QRCAnalyzer()
    
    # Single capacity
    eta = 10
    capacity_train, capacity_test = analyzer.compute_capacity(
        features_df, time_series, eta, f_p=True
    )
    print(f"Capacity for eta = {eta}: {np.round(capacity_test, 4)}")
    
    # Sum capacity
    eta_max = 20
    sum_capacity = analyzer.compute_sum_capacity(
        features_df, time_series, eta_max, f_p=True
    )
    print(f"Sum capacity for eta_max = {eta_max}: {np.round(sum_capacity, 4)}")

def get_data_loaders(dataset_file, batch_size=32):
    """è·å–æ•°æ®åŠ è½½å™¨"""
    with open(dataset_file, 'rb') as file:
        (X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor) = pickle.load(file)

    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
    
    return train_loader, test_loader


def process_schemes_qrc_data(g=0.5, h=1, verbose=True):
    """
    ä½¿ç”¨QRCå¤„ç†schemes_qrc_olsä¸­çš„æ—¶é—´åºåˆ—æ•°æ®
    
    Args:
        g: æµ‹é‡å¼ºåº¦å‚æ•°
        h: ç£åœºå¼ºåº¦å‚æ•° 
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        
    Returns:
        train_features: è®­ç»ƒé›†çš„81ç»´ç‰¹å¾çŸ©é˜µ
        train_targets: è®­ç»ƒé›†ç›®æ ‡å€¼
        test_features: æµ‹è¯•é›†çš„81ç»´ç‰¹å¾çŸ©é˜µ
        test_targets: æµ‹è¯•é›†ç›®æ ‡å€¼
    """ 
    
    if verbose:
        print("=== ä½¿ç”¨QRCå¤„ç†schemes_qrc_olsæ—¶é—´åºåˆ—æ•°æ® ===")
        print(f"å‚æ•°è®¾ç½®: g={g}, h={h}")
    
    # è·å–æ•°æ®åŠ è½½å™¨
    train_loader, test_loader = get_data_loaders('data/Santa_Fe_2000', batch_size=1)  # batch_size=1æ–¹ä¾¿å¤„ç†

    # åˆå§‹åŒ–QRCæ¨¡æ‹Ÿå™¨
    simulator = QRCSimulator(total_spins=6, input_sys_size=1, T_evolution=10)
    
    # å¤„ç†è®­ç»ƒé›†
    print("\n--- å¤„ç†è®­ç»ƒé›† ---")
    train_features_list = []
    train_targets_list = []
    
    train_count = 0
    for batch_idx, (data, target) in enumerate(train_loader):
        # data shape: [1, 6], target shape: [1]
        time_series = data.squeeze().numpy().flatten()  # è½¬æ¢ä¸º [6] çš„numpyæ•°ç»„
        target_value = target.item()       
        
        # ç¡®ä¿æ•°æ®åœ¨[0,1]èŒƒå›´å†…ï¼ˆQRCæœŸæœ›çš„è¾“å…¥èŒƒå›´ï¼‰
        # time_series_normalized = (time_series - time_series.min()) / (time_series.max() - time_series.min() + 1e-8)
        time_series_normalized = time_series
        
        try:
            # ä½¿ç”¨QRCæ¨¡æ‹Ÿå™¨å¤„ç†è¿™6ä¸ªæ—¶é—´ç‚¹
            features_df = simulator.run_simulation(time_series_normalized, g=g, h=h, verbose=False)
            
            # features_dfåº”è¯¥æœ‰6è¡Œï¼Œæ¯è¡Œ81ä¸ªç‰¹å¾
            # æˆ‘ä»¬å¯ä»¥é€‰æ‹©ä¸åŒçš„èšåˆç­–ç•¥ï¼š
            # é€‰é¡¹1: å–æœ€åä¸€è¡Œä½œä¸ºæ•´ä¸ªåºåˆ—çš„ç‰¹å¾è¡¨ç¤º
            # final_features = features_df.iloc[-1].values

            # æ‰€æœ‰è¾“å…¥çš„ZæœŸæœ›
            final_features = features_df.iloc[:, 12:18].values.squeeze().flatten() 
            train_features_list.append(final_features)
            train_targets_list.append(target_value)
            
        except Exception as e:
            if verbose and train_count < 5:  # åªæ˜¾ç¤ºå‰å‡ ä¸ªé”™è¯¯
                print(f"  æ ·æœ¬ {train_count+1} å¤„ç†å¤±è´¥: {e}")
            continue
        
        train_count += 1
        if train_count % 100 == 0 and verbose:
            print(f"  å·²å¤„ç† {train_count} ä¸ªè®­ç»ƒæ ·æœ¬")
       
    
    print(f"è®­ç»ƒé›†å¤„ç†å®Œæˆï¼Œå…± {len(train_features_list)} ä¸ªæœ‰æ•ˆæ ·æœ¬")
    
    # å¤„ç†æµ‹è¯•é›†
    print("\n--- å¤„ç†æµ‹è¯•é›† ---")
    test_features_list = []
    test_targets_list = []
    
    test_count = 0
    for batch_idx, (data, target) in enumerate(test_loader):
        # data shape: [1, 6], target shape: [1]
        time_series = data.squeeze().numpy().flatten()  # è½¬æ¢ä¸º [6] çš„numpyæ•°ç»„
        target_value = target.item()
        
        # ç¡®ä¿æ•°æ®åœ¨[0,1]èŒƒå›´å†…
        # time_series_normalized = (time_series - time_series.min()) / (time_series.max() - time_series.min() + 1e-8)
        time_series_normalized = time_series
        
        try:
            # ä½¿ç”¨QRCæ¨¡æ‹Ÿå™¨å¤„ç†è¿™6ä¸ªæ—¶é—´ç‚¹
            features_df = simulator.run_simulation(time_series_normalized, g=g, h=h, verbose=False)
            
            # å–æœ€åä¸€è¡Œä½œä¸ºç‰¹å¾è¡¨ç¤º
            # final_features = features_df.iloc[-1].values

            final_features = features_df.iloc[:, 12:18].values.squeeze().flatten()
            
            test_features_list.append(final_features)
            test_targets_list.append(target_value)
            
        except Exception as e:
            if verbose and test_count < 5:  # åªæ˜¾ç¤ºå‰å‡ ä¸ªé”™è¯¯
                print(f"  æµ‹è¯•æ ·æœ¬ {test_count+1} å¤„ç†å¤±è´¥: {e}")
            continue
        
        test_count += 1
        if test_count % 100 == 0 and verbose:
            print(f"  å·²å¤„ç† {test_count} ä¸ªæµ‹è¯•æ ·æœ¬")
    
    print(f"æµ‹è¯•é›†å¤„ç†å®Œæˆï¼Œå…± {len(test_features_list)} ä¸ªæœ‰æ•ˆæ ·æœ¬")
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    if len(train_features_list) == 0 or len(test_features_list) == 0:
        raise ValueError("æ²¡æœ‰æˆåŠŸå¤„ç†çš„æ ·æœ¬ï¼Œè¯·æ£€æŸ¥æ•°æ®æ ¼å¼å’Œå‚æ•°è®¾ç½®")
    
    train_features = np.array(train_features_list)  # shape: [N_train, 81]
    train_targets = np.array(train_targets_list)    # shape: [N_train]
    test_features = np.array(test_features_list)    # shape: [N_test, 81]
    test_targets = np.array(test_targets_list)      # shape: [N_test]
    
    if verbose:
        print("\n=== ç‰¹å¾æå–å®Œæˆ ===")
        print(f"è®­ç»ƒé›†ç‰¹å¾å½¢çŠ¶: {train_features.shape}")
        print(f"è®­ç»ƒé›†ç›®æ ‡å½¢çŠ¶: {train_targets.shape}")
        print(f"æµ‹è¯•é›†ç‰¹å¾å½¢çŠ¶: {test_features.shape}")
        print(f"æµ‹è¯•é›†ç›®æ ‡å½¢çŠ¶: {test_targets.shape}")
        print(f"ç‰¹å¾ç»´åº¦: {train_features.shape[1]} (åº”è¯¥æ˜¯81)")
        
        # æ˜¾ç¤ºç‰¹å¾ç»Ÿè®¡ä¿¡æ¯
        print(f"\nç‰¹å¾ç»Ÿè®¡:")
        print(f"  è®­ç»ƒé›†ç‰¹å¾èŒƒå›´: [{train_features.min():.6f}, {train_features.max():.6f}]")
        print(f"  æµ‹è¯•é›†ç‰¹å¾èŒƒå›´: [{test_features.min():.6f}, {test_features.max():.6f}]")
        print(f"  è®­ç»ƒé›†ç›®æ ‡èŒƒå›´: [{train_targets.min():.6f}, {train_targets.max():.6f}]")
        print(f"  æµ‹è¯•é›†ç›®æ ‡èŒƒå›´: [{test_targets.min():.6f}, {test_targets.max():.6f}]")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ— æ•ˆå€¼
        if np.any(np.isnan(train_features)) or np.any(np.isinf(train_features)):
            print("  âš ï¸ è®­ç»ƒé›†ç‰¹å¾ä¸­åŒ…å«NaNæˆ–Infå€¼")
        if np.any(np.isnan(test_features)) or np.any(np.isinf(test_features)):
            print("  âš ï¸ æµ‹è¯•é›†ç‰¹å¾ä¸­åŒ…å«NaNæˆ–Infå€¼")
    
    return train_features, train_targets, test_features, test_targets




def load_qrc_features(filepath):
    """
    ä»æ–‡ä»¶åŠ è½½QRCç‰¹å¾æ•°æ®
    
    Args:
        filepath: æ–‡ä»¶è·¯å¾„
        
    Returns:
        train_features, train_targets, test_features, test_targets
    """
    import pickle
    
    if filepath.endswith('.npz'):
        # åŠ è½½npzæ ¼å¼
        data = np.load(filepath)
        train_features = data['train_features']
        train_targets = data['train_targets']
        test_features = data['test_features']
        test_targets = data['test_targets']
        
    elif filepath.endswith('.pkl'):
        # åŠ è½½pickleæ ¼å¼
        with open(filepath, 'rb') as f:
            data_dict = pickle.load(f)
        train_features = data_dict['train_features']
        train_targets = data_dict['train_targets']
        test_features = data_dict['test_features']
        test_targets = data_dict['test_targets']
        
        # æ˜¾ç¤ºå…ƒæ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
        if 'metadata' in data_dict:
            metadata = data_dict['metadata']
            print("åŠ è½½çš„æ•°æ®ä¿¡æ¯:")
            for key, value in metadata.items():
                print(f"  {key}: {value}")
                
    elif filepath.endswith('.csv'):
        # åŠ è½½CSVæ ¼å¼
        df = pd.read_csv(filepath)
        train_df = df[df['split'] == 'train']
        test_df = df[df['split'] == 'test']
        
        # æå–ç‰¹å¾å’Œç›®æ ‡
        feature_cols = [col for col in df.columns if col not in ['target', 'split']]
        train_features = train_df[feature_cols].values
        train_targets = train_df['target'].values
        test_features = test_df[feature_cols].values
        test_targets = test_df['target'].values
        
    else:
        raise ValueError("Unsupported file format. Use .npz, .pkl, or .csv")
    
    print(f"âœ… å·²ä» {filepath} åŠ è½½æ•°æ®")
    print(f"è®­ç»ƒé›†: {train_features.shape[0]} æ ·æœ¬, {train_features.shape[1]} ç‰¹å¾")
    print(f"æµ‹è¯•é›†: {test_features.shape[0]} æ ·æœ¬, {test_features.shape[1]} ç‰¹å¾")
    
    return train_features, train_targets, test_features, test_targets


def demo_schemes_qrc_integration(filename):
    """æ¼”ç¤ºQRCä¸schemes_qrc_olsçš„é›†æˆ"""
    print("QRC + schemes_qrc_ols é›†æˆæ¼”ç¤º")
    print("=" * 50)
    
    # å¤„ç†æ•°æ®  éœ€è¦æ‰‹åŠ¨æŒ‡å®šæ–‡ä»¶å
    train_features, train_targets, test_features, test_targets = process_schemes_qrc_data(
        g=0.5, h=1, verbose=True
    )
        
    # æ˜¾ç¤ºå‡ ä¸ªæ ·æœ¬
    print(f"\nå‰3ä¸ªè®­ç»ƒæ ·æœ¬çš„ç‰¹å¾(å‰5ç»´):")
    for i in range(min(3, len(train_features))):
        print(f"  æ ·æœ¬{i+1}: {train_features[i][:5]} -> ç›®æ ‡: {train_targets[i]:.6f}")
    
    # ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶
    print(f"\n" + "="*50)
    save_qrc_features(train_features, train_targets, test_features, test_targets,
                     filename=filename, save_format='npz')

    # æ¼”ç¤ºåŠ è½½æ•°æ®
    print(f"\n" + "="*30)
    print("æ¼”ç¤ºæ•°æ®åŠ è½½:")
    loaded_train_features, loaded_train_targets, loaded_test_features, loaded_test_targets = load_qrc_features(filename+".npz")

    # éªŒè¯æ•°æ®ä¸€è‡´æ€§
    print(f"\næ•°æ®ä¸€è‡´æ€§æ£€æŸ¥:")
    print(f"è®­ç»ƒç‰¹å¾ä¸€è‡´: {np.array_equal(train_features, loaded_train_features)}")
    print(f"è®­ç»ƒç›®æ ‡ä¸€è‡´: {np.array_equal(train_targets, loaded_train_targets)}")
    print(f"æµ‹è¯•ç‰¹å¾ä¸€è‡´: {np.array_equal(test_features, loaded_test_features)}")
    print(f"æµ‹è¯•ç›®æ ‡ä¸€è‡´: {np.array_equal(test_targets, loaded_test_targets)}")    


def evaluate_qrc_regression(filename):
    """
    ä»ä¿å­˜çš„QRCç‰¹å¾æ–‡ä»¶ä¸­è¯»å–æ•°æ®ï¼Œä½¿ç”¨çº¿æ€§å›å½’è¿›è¡Œæ‹Ÿåˆï¼Œå¹¶è®¡ç®—capacity
    """
    print("=== QRCç‰¹å¾çº¿æ€§å›å½’è¯„ä¼° ===")
    
    # 1. åŠ è½½ä¿å­˜çš„QRCç‰¹å¾æ•°æ®
    try:
        train_features, train_targets, test_features, test_targets = load_qrc_features(filename)
    except FileNotFoundError:
        print(f"âŒ æœªæ‰¾åˆ° {filename} æ–‡ä»¶")
        print("è¯·å…ˆè¿è¡Œ demo_schemes_qrc_integration() ç”Ÿæˆç‰¹å¾æ–‡ä»¶")
        return None
    
    print(f"\næ•°æ®åŠ è½½å®Œæˆ:")
    print(f"  è®­ç»ƒé›†: {train_features.shape[0]} æ ·æœ¬, {train_features.shape[1]} ç‰¹å¾")
    print(f"  æµ‹è¯•é›†: {test_features.shape[0]} æ ·æœ¬, {test_features.shape[1]} ç‰¹å¾")
    
    # 2. ä½¿ç”¨çº¿æ€§å›å½’æ‹Ÿåˆè®­ç»ƒé›†
    print(f"\n--- è®­ç»ƒçº¿æ€§å›å½’æ¨¡å‹ ---")
    model = LinearRegression()
    model.fit(train_features, train_targets)
    
    print(f"âœ… çº¿æ€§å›å½’æ¨¡å‹è®­ç»ƒå®Œæˆ")
    print(f"æ¨¡å‹ç³»æ•°ç»´åº¦: {model.coef_.shape}")
    print(f"æ¨¡å‹æˆªè·: {model.intercept_:.6f}")
    
    # 3. è¿›è¡Œé¢„æµ‹
    train_predictions = model.predict(train_features)
    test_predictions = model.predict(test_features)
    
    print(f"\n--- é¢„æµ‹ç»“æœç»Ÿè®¡ ---")
    print(f"è®­ç»ƒé›†é¢„æµ‹èŒƒå›´: [{train_predictions.min():.6f}, {train_predictions.max():.6f}]")
    print(f"æµ‹è¯•é›†é¢„æµ‹èŒƒå›´: [{test_predictions.min():.6f}, {test_predictions.max():.6f}]")
    print(f"è®­ç»ƒé›†çœŸå€¼èŒƒå›´: [{train_targets.min():.6f}, {train_targets.max():.6f}]")
    print(f"æµ‹è¯•é›†çœŸå€¼èŒƒå›´: [{test_targets.min():.6f}, {test_targets.max():.6f}]")
    
    # 4. è®¡ç®—capacityï¼ˆæŒ‰ç…§QRCçš„å®šä¹‰ï¼‰
    def compute_capacity(y_true, y_pred):
        """
        è®¡ç®—capacity = (Cov(y_true, y_pred))^2 / (Var(y_true) * Var(y_pred))
        è¿™æ˜¯QRCä¸­ä½¿ç”¨çš„capacityå®šä¹‰
        """
        # ç¡®ä¿æ˜¯numpyæ•°ç»„
        y_true = np.array(y_true).flatten()
        y_pred = np.array(y_pred).flatten()
        
        # è®¡ç®—åæ–¹å·®å’Œæ–¹å·®
        covariance = np.cov(y_true, y_pred, ddof=1)[0, 1]
        var_true = np.var(y_true, ddof=1)
        var_pred = np.var(y_pred, ddof=1)
        
        # è®¡ç®—capacity
        if var_true == 0 or var_pred == 0:
            return 0.0
        
        capacity = (covariance ** 2) / (var_true * var_pred)
        return capacity
    
    # 5. è®¡ç®—è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„capacity
    train_capacity = compute_capacity(train_targets, train_predictions)
    test_capacity = compute_capacity(test_targets, test_predictions)
    
    print(f"\n=== Capacity è¯„ä¼°ç»“æœ ===")
    print(f"è®­ç»ƒé›† Capacity: {train_capacity:.6f}")
    print(f"æµ‹è¯•é›† Capacity:  {test_capacity:.6f}")
    
    # 6. é¢å¤–çš„æ€§èƒ½æŒ‡æ ‡
    from sklearn.metrics import r2_score, mean_squared_error
    
    train_r2 = r2_score(train_targets, train_predictions)
    test_r2 = r2_score(test_targets, test_predictions)
    train_mse = mean_squared_error(train_targets, train_predictions)
    test_mse = mean_squared_error(test_targets, test_predictions)
    
    print(f"\n=== é¢å¤–æ€§èƒ½æŒ‡æ ‡ ===")
    print(f"è®­ç»ƒé›† RÂ²: {train_r2:.6f}")
    print(f"æµ‹è¯•é›† RÂ²:  {test_r2:.6f}")
    print(f"è®­ç»ƒé›† MSE: {train_mse:.6f}")
    print(f"æµ‹è¯•é›† MSE:  {test_mse:.6f}")
    
    # 7. ä¿å­˜ç»“æœ
    results = {
        'train_capacity': train_capacity,
        'test_capacity': test_capacity,
        'train_r2': train_r2,
        'test_r2': test_r2,
        'train_mse': train_mse,
        'test_mse': test_mse,
        'model_coefficients': model.coef_,
        'model_intercept': model.intercept_,
        'train_predictions': train_predictions,
        'test_predictions': test_predictions
    }  
    
    # 8. å¯è§†åŒ–ç»“æœï¼ˆå¯é€‰ï¼‰
    try:
        import matplotlib.pyplot as plt
        
        fig, axes = plt.subplots(1, 2, figsize=(12, 5))
        
        # è®­ç»ƒé›†é¢„æµ‹ vs çœŸå€¼
        axes[0].scatter(train_targets, train_predictions, alpha=0.6, s=10)
        axes[0].plot([train_targets.min(), train_targets.max()], 
                    [train_targets.min(), train_targets.max()], 'r--', lw=2)
        axes[0].set_xlabel('True Values')
        axes[0].set_ylabel('Predictions')
        axes[0].set_title(f'Training Set\nCapacity = {train_capacity:.4f}')
        axes[0].grid(True, alpha=0.3)
        
        # æµ‹è¯•é›†é¢„æµ‹ vs çœŸå€¼
        axes[1].scatter(test_targets, test_predictions, alpha=0.6, s=10)
        axes[1].plot([test_targets.min(), test_targets.max()], 
                    [test_targets.min(), test_targets.max()], 'r--', lw=2)
        axes[1].set_xlabel('True Values')
        axes[1].set_ylabel('Predictions')
        axes[1].set_title(f'Test Set\nCapacity = {test_capacity:.4f}')
        axes[1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plot_file = f"qrc_regression_plot.png"
        plt.savefig(plot_file, dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"ğŸ“Š å›¾è¡¨å·²ä¿å­˜åˆ°: {plot_file}")
        
    except ImportError:
        print("âš ï¸ matplotlibæœªå®‰è£…ï¼Œè·³è¿‡å¯è§†åŒ–")
    
    return results


# æ·»åŠ åˆ°mainå‡½æ•°çš„è°ƒç”¨ç¤ºä¾‹
def test_qrc_evaluation():
    """æµ‹è¯•QRCè¯„ä¼°åŠŸèƒ½"""
    print("å¼€å§‹QRCè¯„ä¼°æµ‹è¯•...")
    
    # å¦‚æœç‰¹å¾æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå…ˆç”Ÿæˆ
    filename = "data/santa_fe_features"
    if not os.path.exists(filename + ".npz"):
        print("ç‰¹å¾æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå…ˆç”ŸæˆQRCç‰¹å¾...")
        demo_schemes_qrc_integration(filename)

    # è¿è¡Œè¯„ä¼°
    results = evaluate_qrc_regression(filename + ".npz")
    
    if results:
        print(f"\nğŸ‰ QRCè¯„ä¼°å®Œæˆ!")
        print(f"ä¸»è¦ç»“æœ: æµ‹è¯•é›†Capacity = {results['test_capacity']:.6f}")


if __name__ == "__main__":
    # åŸæœ‰çš„mainå‡½æ•°
    # main()

    # demo_schemes_qrc_integration()
    
    # æ–°å¢çš„QRCè¯„ä¼°ä»»åŠ¡
    test_qrc_evaluation()
