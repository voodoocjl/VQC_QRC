import pickle
import os
import random
import json
import csv
import numpy as np
import torch
import time
from schemes_qrc_ols import Scheme_Quantum_Escape, Scheme_eval, Scheme_enhanced
from FusionModel import single_enta_to_design

n_qubits = 6
n_layers = 2

path = ['search_space/search_space_qrc_6_single', 'search_space/search_space_qrc_6_enta']

checkpoint = 'init_weights/init_weight_qrc'

epochs = 30
periods = 6
split = 3

# æ–­ç‚¹ç»­ä¼ æ–‡ä»¶
checkpoint_file = 'random_search_checkpoint.pkl'

def save_checkpoint(epoch, period, single, enta, weights, search_space_single, search_space_enta, global_id_counter):
    """ä¿å­˜æœç´¢çŠ¶æ€çš„æ£€æŸ¥ç‚¹"""
    checkpoint_data = {
        'epoch': epoch,
        'period': period,
        'single': single,
        'enta': enta,
        'weights': weights,
        'search_space_single': search_space_single,
        'search_space_enta': search_space_enta,
        'global_id_counter': global_id_counter,
        'timestamp': time.time(),
        'n_qubits': n_qubits,
        'n_layers': n_layers,
        'epochs': epochs,
        'periods': periods,
        'split': split
    }
    
    with open(checkpoint_file, 'wb') as f:
        pickle.dump(checkpoint_data, f)
    
    print(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜: Epoch {epoch+1}, Period {period+1}")

def load_checkpoint():
    """åŠ è½½æœç´¢çŠ¶æ€çš„æ£€æŸ¥ç‚¹"""
    if os.path.exists(checkpoint_file):
        try:
            with open(checkpoint_file, 'rb') as f:
                checkpoint_data = pickle.load(f)
            
            print(f"å‘ç°æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼Œæ¢å¤çŠ¶æ€...")
            print(f"  ä¸Šæ¬¡ä¿å­˜æ—¶é—´: {time.ctime(checkpoint_data['timestamp'])}")
            print(f"  ä¸Šæ¬¡è¿›åº¦: Epoch {checkpoint_data['epoch']+1}, Period {checkpoint_data['period']+1}")
            
            # éªŒè¯å‚æ•°ä¸€è‡´æ€§
            if (checkpoint_data['n_qubits'] != n_qubits or 
                checkpoint_data['n_layers'] != n_layers or
                checkpoint_data['epochs'] != epochs or
                checkpoint_data['periods'] != periods or
                checkpoint_data['split'] != split):
                print("è­¦å‘Š: æ£€æŸ¥ç‚¹å‚æ•°ä¸å½“å‰è®¾ç½®ä¸ä¸€è‡´ï¼Œå°†é‡æ–°å¼€å§‹")
                return None
            
            return checkpoint_data
            
        except Exception as e:
            print(f"åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
            print("å°†é‡æ–°å¼€å§‹æœç´¢")
            return None
    else:
        print("æœªå‘ç°æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼Œä»å¤´å¼€å§‹æœç´¢")
        return None

def initialize_or_restore():
    """åˆå§‹åŒ–æˆ–æ¢å¤æœç´¢çŠ¶æ€"""
    checkpoint_data = load_checkpoint()
    
    if checkpoint_data is not None:
        # ä»æ£€æŸ¥ç‚¹æ¢å¤
        start_epoch = checkpoint_data['epoch']
        start_period = checkpoint_data['period'] + 1  # ä»ä¸‹ä¸€ä¸ªperiodå¼€å§‹
        single = checkpoint_data['single']
        enta = checkpoint_data['enta']
        weights = checkpoint_data['weights']
        search_space_single = checkpoint_data['search_space_single']
        search_space_enta = checkpoint_data['search_space_enta']
        global_id_counter = checkpoint_data['global_id_counter']
        
        # å¦‚æœperiodè¶…å‡ºèŒƒå›´ï¼Œè¿›å…¥ä¸‹ä¸€ä¸ªepoch
        if start_period >= periods:
            start_epoch += 1
            start_period = 0
        
        print(f"ä» Epoch {start_epoch+1}, Period {start_period+1} ç»§ç»­æœç´¢")
        
        # æ£€æŸ¥CSVæ–‡ä»¶æ˜¯å¦éœ€è¦åˆ›å»ºè¡¨å¤´
        if not os.path.exists(results_file):
            with open(results_file, 'w', newline='') as csvfile:
                writer = csv.writer(csvfile)
                writer.writerow(['num_id', 'job', 'epoch', 'period', 'component_type', 'train_capacity', 'test_capacity'])
        
        if not os.path.exists(complete_results_file):
            with open(complete_results_file, 'w', newline='') as csvfile:
                writer = csv.writer(csvfile)
                writer.writerow(['network_structure', 'train_capacity', 'test_capacity'])
                
    else:
        # å…¨æ–°å¼€å§‹
        start_epoch = 0
        start_period = 0
        single = [[i]+[1]*1*n_layers for i in range(1,n_qubits+1)]
        enta = [[i]+[i+1]*n_layers for i in range(1,n_qubits)]+[[n_qubits]+[1]*n_layers]
        weights = torch.load(checkpoint)
        global_id_counter = 0
        
        # åŠ è½½åˆå§‹æœç´¢ç©ºé—´
        with open(path[0], 'rb') as file:
            search_space_single = pickle.load(file)        
        with open(path[1], 'rb') as file:
            search_space_enta = pickle.load(file)
        
        # åˆå§‹åŒ–CSVæ–‡ä»¶
        with open(results_file, 'w', newline='') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(['num_id', 'job', 'epoch', 'period', 'component_type', 'train_capacity', 'test_capacity'])
        
        with open(complete_results_file, 'w', newline='') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(['network_structure', 'train_capacity', 'test_capacity'])
        
        print("å¼€å§‹æ–°çš„æœç´¢")
    
    return start_epoch, start_period, single, enta, weights, search_space_single, search_space_enta, global_id_counter

# åœ¨æ–‡ä»¶å¼€å¤´å®šä¹‰æ–‡ä»¶å
results_file = 'results_random_search.csv'
complete_results_file = 'complete_network_results.csv'

# å…¶ä»–å‡½æ•°ä¿æŒä¸å˜...
def remove_matching_items(search_list, target_item):
    """åˆ é™¤åˆ—è¡¨ä¸­ä¸target_itemå†…å®¹ç›¸åŒçš„æ‰€æœ‰å…ƒç´ """
    filtered_list = [item for item in search_list if item not in target_item]
    return filtered_list

def insert_job(change_code, job_input):
    """å‚è€ƒMCTSä¸­çš„insert_jobæ–¹æ³•"""
    import copy
    job = copy.deepcopy(job_input)
    if type(job[0]) == type([]):
        qubit = [sub[0] for sub in job]
    else:
        qubit = [job[0]]
        job = [job]
    if change_code != None:            
        for change in change_code:
            if change[0] not in qubit:
                job.append(change)
    return job

def create_designs_from_search_space(search_space, base_components, component_type):
    """é€šç”¨å‡½æ•°ï¼šä»æœç´¢ç©ºé—´åˆ›å»ºè®¾è®¡"""
    jobs = []
    designs = []
    
    for candidate_item in search_space:
        if type(candidate_item[0]) != type([]):
            job = [candidate_item]
        else:
            job = candidate_item
            
        if component_type == 'single':
            integrated_single = insert_job(base_components['single'], job)
            integrated_enta = base_components['enta']
        elif component_type == 'enta':
            integrated_single = base_components['single']
            integrated_enta = insert_job(base_components['enta'], job)
        else:
            raise ValueError("component_type must be 'single' or 'enta'")
        
        design = single_enta_to_design(integrated_single, integrated_enta, (n_qubits, n_layers))        
        jobs.append(job)
        designs.append(design)        
    
    return jobs, designs

def evaluate_designs(designs, jobs, weights, max_evaluations=None):
    """è¯„ä¼°ç”Ÿæˆçš„è®¾è®¡"""
    results = []
    
    if max_evaluations is None:
        max_evaluations = len(designs)
    else:
        max_evaluations = min(max_evaluations, len(designs))
    
    print(f"å¼€å§‹è¯„ä¼° {max_evaluations} ä¸ªè®¾è®¡...")
    
    for i in range(max_evaluations):
        if designs[i] is None:
            print(f"  è®¾è®¡ {i+1}: æ— æ•ˆè®¾è®¡ï¼Œè·³è¿‡")
            results.append(None)
            continue
            
        print(f"  è¯„ä¼°è®¾è®¡ {i+1}/{max_evaluations}")        
        print(f"    ä»»åŠ¡: {jobs[i]}")        
    
        result = Scheme_eval(designs[i], weight=weights)
        result['job'] = jobs[i]
        result['design_index'] = i
        results.append(result)
    return results

def save_results(results, epoch, period, component_type):
    """ä¿å­˜è¯„ä¼°ç»“æœåˆ°ç»Ÿä¸€çš„CSVæ–‡ä»¶"""
    global global_id_counter
    
    valid_results = [r for r in results if r is not None]
    
    if len(valid_results) == 0:
        print("æ²¡æœ‰æœ‰æ•ˆç»“æœå¯ä¿å­˜")
        return
    
    with open(results_file, 'a', newline='') as csvfile:
        writer = csv.writer(csvfile)
        
        for result in valid_results:
            global_id_counter += 1
            job_str = str(result['job'])
            train_capacity = result.get('train_capacity', 0.0)
            test_capacity = result.get('capacity', 0.0)
            
            writer.writerow([
                global_id_counter, job_str, epoch + 1, period + 1,
                component_type, train_capacity, test_capacity
            ])
    
    print(f"å·²è¿½åŠ  {len(valid_results)} æ¡è®°å½•åˆ° {results_file}")

def save_complete_network_result(single, enta, result):
    """ä¿å­˜å®Œæ•´ç½‘ç»œç»“æ„å’Œç»“æœåˆ°CSVæ–‡ä»¶"""
    network_structure = [single, enta]
    network_str = str(network_structure)
    train_capacity = result.get('train_capacity', 0.0)
    test_capacity = result.get('capacity', 0.0)
    
    with open(complete_results_file, 'a', newline='') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow([network_str, train_capacity, test_capacity])
    
    print(f"å·²ä¿å­˜å®Œæ•´ç½‘ç»œç»“æ„åˆ° {complete_results_file}")
    print(f"Train Capacity: {train_capacity:.6f}")
    print(f"Test Capacity: {test_capacity:.6f}")

# ä¸»å‡½æ•°
def main():
    global global_id_counter
    
    try:
        # åˆå§‹åŒ–æˆ–æ¢å¤çŠ¶æ€
        start_epoch, start_period, single, enta, weights, search_space_single, search_space_enta, global_id_counter = initialize_or_restore()
        
        # ä¸»æœç´¢å¾ªç¯
        for i in range(start_epoch, epochs):
            # å¦‚æœæ˜¯æ¢å¤çš„epochï¼Œä»æŒ‡å®šperiodå¼€å§‹ï¼›å¦åˆ™ä»0å¼€å§‹
            period_start = start_period if i == start_epoch else 0

            # åŠ è½½åˆå§‹æœç´¢ç©ºé—´
            with open(path[0], 'rb') as file:
                search_space_single = pickle.load(file)        
            with open(path[1], 'rb') as file:
                search_space_enta = pickle.load(file)
            
            
            for j in range(period_start, periods):
                print(f"\n{'='*60}")
                print(f"Epoch {i+1}/{epochs}, Period {j+1}/{periods}")
                print(f"{'='*60}")
                
                base_components = {'single': single, 'enta': enta}

                if j < split:
                    # å¤„ç†singleæœç´¢ç©ºé—´
                    print(f"\n--- å¤„ç†Singleæœç´¢ç©ºé—´ ---")
                    search_space_single = remove_matching_items(search_space_single, single)
                    print(f"å¯ç”¨çš„singleå€™é€‰æ•°é‡: {len(search_space_single)}")
                
                    if len(search_space_single) > 0:
                        single_space = random.sample(search_space_single, min(10, len(search_space_single)))
                        jobs_single, designs_single = create_designs_from_search_space(
                            single_space, base_components, 'single'
                        )
                        
                        print(f"ç”Ÿæˆäº† {len(designs_single)} ä¸ªsingleè®¾è®¡")
                        
                        results = evaluate_designs(designs_single, jobs_single, weights, max_evaluations=10)
                        save_results(results, i, j, 'single')

                        valid_results = [r for r in results if r is not None]
                        if len(valid_results) > 0:                
                            max_capacity = max(r.get('capacity', 0.0) for r in valid_results)                
                            best_results = [r for r in valid_results if r.get('capacity', 0.0) == max_capacity]
                            
                            best_result = random.choice(best_results)
                            best_job = best_result['job']
                            
                            print(f"æœ€ä½³test_capacity: {max_capacity:.6f}")
                            print(f"æ‰¾åˆ° {len(best_results)} ä¸ªæœ€ä½³ç»“æœï¼Œéšæœºé€‰æ‹©: {best_job}")
                        else:
                            best_job = None
                            print("æ²¡æœ‰æœ‰æ•ˆçš„singleç»“æœ")

                        if best_job is not None:
                            single = insert_job(single, best_job)
                            qubit_used = best_job[0][0]
                            print(f"ä½¿ç”¨çš„qubit: {qubit_used}")            
                            search_space_single = [item for item in search_space_single if item[0] != qubit_used]

                        if j == split - 1:
                            design = single_enta_to_design(single, enta, (n_qubits, n_layers))
                            result = Scheme_eval(design, weights)

                            ols_regressor = result.get('regressor', None)
                            if ols_regressor is not None:
                                fc_weight = torch.tensor(ols_regressor.coef_, dtype=torch.float32, requires_grad=True).unsqueeze(0)
                                fc_bias = torch.tensor([ols_regressor.intercept_], dtype=torch.float32, requires_grad=True)
                                weights['fc.weight'] = fc_weight
                                weights['fc.bias'] = fc_bias

                                print("å¼€å§‹é‡å­é€ƒé€¸è®­ç»ƒ...")
                                # best_model, report = Scheme_Quantum_Escape((n_qubits, n_layers), design, weight=weights, epochs=30)
                                best_model, report = Scheme_enhanced((n_qubits, n_layers), design, weight=weights, epochs=30)
                                weights = best_model.state_dict()

                                result = Scheme_eval(design, weights, draw=True)
                                save_complete_network_result(single, enta, result)

                else:
                    # å¤„ç†entaæœç´¢ç©ºé—´
                    print(f"\n--- å¤„ç†Entaæœç´¢ç©ºé—´ ---")
                    search_space_enta = remove_matching_items(search_space_enta, enta)
                    print(f"å¯ç”¨çš„entaå€™é€‰æ•°é‡: {len(search_space_enta)}")
                    
                    if len(search_space_enta) > 0:
                        enta_space = random.sample(search_space_enta, min(10, len(search_space_enta)))
                        jobs_enta, designs_enta = create_designs_from_search_space(
                            enta_space, base_components, 'enta'
                        )
                        
                        print(f"ç”Ÿæˆäº† {len(designs_enta)} ä¸ªentaè®¾è®¡")
                        
                        results = evaluate_designs(designs_enta, jobs_enta, weights, max_evaluations=10)
                        save_results(results, i, j, 'enta')

                        valid_results = [r for r in results if r is not None]
                        if len(valid_results) > 0:                
                            max_capacity = max(r.get('capacity', 0.0) for r in valid_results)                
                            best_results = [r for r in valid_results if r.get('capacity', 0.0) == max_capacity]
                            
                            best_result = random.choice(best_results)
                            best_job = best_result['job']
                            
                            print(f"æœ€ä½³test_capacity: {max_capacity:.6f}")
                            print(f"æ‰¾åˆ° {len(best_results)} ä¸ªæœ€ä½³ç»“æœï¼Œéšæœºé€‰æ‹©: {best_job}")
                            
                            enta = insert_job(enta, best_job)
                            print(f"æ›´æ–°åçš„enta: {enta}")
                            
                            qubit_used = best_job[0][0]
                            print(f"ä½¿ç”¨çš„qubit: {qubit_used}")
                            search_space_enta = [item for item in search_space_enta if item[0] != qubit_used]                
                            
                        else:
                            best_job = None
                            print("æ²¡æœ‰æœ‰æ•ˆçš„entaç»“æœ")

                        if j == periods - 1:
                            design = single_enta_to_design(single, enta, (n_qubits, n_layers))                                         
                            result = Scheme_eval(design, weights)
                            ols_regressor = result.get('regressor', None)
                            if ols_regressor is not None:
                                fc_weight = torch.tensor(ols_regressor.coef_, dtype=torch.float32, requires_grad=True).unsqueeze(0)
                                fc_bias = torch.tensor([ols_regressor.intercept_], dtype=torch.float32, requires_grad=True)
                                weights['fc.weight'] = fc_weight
                                weights['fc.bias'] = fc_bias

                                print("å¼€å§‹é‡å­é€ƒé€¸è®­ç»ƒ...")
                                # best_model, report = Scheme_Quantum_Escape((n_qubits, n_layers), design, weight=weights, epochs=30)
                                best_model, report = Scheme_enhanced((n_qubits, n_layers), design, weight=weights, epochs=30)
                                weights = best_model.state_dict()

                                result = Scheme_eval(design, weights, draw=True)                     
                                save_complete_network_result(single, enta, result)

                # æ¯ä¸ªperiodç»“æŸåä¿å­˜æ£€æŸ¥ç‚¹
                save_checkpoint(i, j, single, enta, weights, search_space_single, search_space_enta, global_id_counter)

        print(f"\nğŸ‰ éšæœºæœç´¢å®Œæˆ!")
        
        # æœç´¢å®Œæˆååˆ é™¤æ£€æŸ¥ç‚¹æ–‡ä»¶
        if os.path.exists(checkpoint_file):
            os.remove(checkpoint_file)
            print("æœç´¢å®Œæˆï¼Œå·²æ¸…ç†æ£€æŸ¥ç‚¹æ–‡ä»¶")
            
    except KeyboardInterrupt:
        print(f"\n\nâš ï¸  æœç´¢è¢«ç”¨æˆ·ä¸­æ–­")
        print(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜ï¼Œå¯ä»¥é€šè¿‡é‡æ–°è¿è¡Œç¨‹åºç»§ç»­æœç´¢")
        
    except Exception as e:
        print(f"\n\nâŒ æœç´¢è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜ï¼Œå¯ä»¥é€šè¿‡é‡æ–°è¿è¡Œç¨‹åºç»§ç»­æœç´¢")
        raise

if __name__ == "__main__":
    main()
