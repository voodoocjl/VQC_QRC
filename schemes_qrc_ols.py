import copy
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import time
import pickle
import random
from FusionModel import single_enta_to_design, Cell
from sklearn.preprocessing import MinMaxScaler
import pandas as pd
from datetime import datetime

# å¼ºåˆ¶ç¦ç”¨CUDA
torch.cuda.is_available = lambda: False

seq_length = 6  # é¢„æµ‹æ­¥é•¿
n_layers = 2
n_qubits = 6


def get_param_num(model):
    total_num = sum(p.numel() for p in model.parameters())
    trainable_num = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print('total:', total_num, 'trainable:', trainable_num)


def display(metrics):
    RED = '\033[31m'
    GREEN = '\033[32m'
    YELLOW = '\033[33m'
    BLUE = '\033[34m'
    MAGENTA = '\033[35m'
    CYAN = '\033[36m'
    WHITE = '\033[37m'
    RESET = '\033[0m'

    print(YELLOW + "\nTest NMSE: {:.6f}".format(metrics) + RESET)


def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True


def get_data_loaders(batch_size=32):
    """è·å–æ•°æ®åŠ è½½å™¨"""
    # with open('data/Santa_Fe_200', 'rb') as file:
    #     (X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor) = pickle.load(file)

    with open('data/stock_price_data', 'rb') as file:
        (X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor) = pickle.load(file)

    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
    
    return train_loader, test_loader


def nmse(y_true, y_pred):
    """è®¡ç®—å½’ä¸€åŒ–å‡æ–¹è¯¯å·® (NMSE)"""
    return np.mean((y_true - y_pred) ** 2) / np.var(y_true)

def compute_capacity(y_true, y_pred, eps=1e-12):
    """è®¡ç®—å®¹é‡ = ç›¸å…³ç³»æ•°çš„å¹³æ–¹"""
    y_true = np.array(y_true).flatten()
    y_pred = np.array(y_pred).flatten()
    
    # è®¡ç®—åæ–¹å·®å’Œæ–¹å·®
    cov = np.cov(y_true, y_pred, ddof=1)[0, 1]
    var_true = np.var(y_true, ddof=1)
    var_pred = np.var(y_pred, ddof=1)
    
    # é¿å…é™¤é›¶
    if var_true <= eps or var_pred <= eps:
        return 0.0
    
    # capacity = ç›¸å…³ç³»æ•°çš„å¹³æ–¹
    capacity = (cov ** 2) / (var_true * var_pred)
    return capacity

def train(model, train_loader, optimizer, criterion, device):
    """è®­ç»ƒæ¨¡å‹ä¸€ä¸ªepoch"""
    model.train()
    running_loss = 0.0
    all_preds = []
    all_targets = []

    for data, target in train_loader:
        data, target = data.to(device), target.to(device)

        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output.squeeze(-1), target)
        loss.backward()

        # æ£€æŸ¥VQCå‚æ•°çš„æ¢¯åº¦
        check_vqc_gradients(model)

        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        optimizer.step()
        
        # æ£€æŸ¥å‚æ•°æ›´æ–°
        check_parameter_updates(model)

        running_loss += loss.item()
        all_preds.append(output.detach().cpu().numpy())
        all_targets.append(target.detach().cpu().numpy())

    # è®¡ç®—æ•´ä¸ªè®­ç»ƒé›†çš„NMSEå’ŒCapacity
    all_preds = np.concatenate(all_preds)
    all_targets = np.concatenate(all_targets)
    train_nmse = nmse(all_targets, all_preds)
    train_capacity = compute_capacity(all_targets, all_preds)

    return running_loss / len(train_loader), train_nmse, train_capacity


def test(model, test_loader, criterion, device):
    """æµ‹è¯•æ¨¡å‹"""
    model.eval()
    running_loss = 0.0
    all_preds = []
    all_targets = []

    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            loss = criterion(output.squeeze(-1), target)

            running_loss += loss.item()
            all_preds.append(output.detach().cpu().numpy())
            all_targets.append(target.detach().cpu().numpy())

    # è®¡ç®—æ•´ä¸ªæµ‹è¯•é›†çš„NMSE
    all_preds = np.concatenate(all_preds)
    all_targets = np.concatenate(all_targets)
    test_nmse = nmse(all_targets, all_preds)
    test_capacity = compute_capacity(all_targets, all_preds)
    
    return running_loss / len(test_loader), test_nmse, test_capacity, all_preds, all_targets


def evaluate(model, data_loader, device):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    model.eval()
    all_preds = []
    all_targets = []
    
    with torch.no_grad():
        for data, target in data_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            all_preds.append(output.detach().cpu().numpy())
            all_targets.append(target.detach().cpu().numpy())

    all_preds = np.concatenate(all_preds)
    all_targets = np.concatenate(all_targets)
    metrics = nmse(all_targets, all_preds)
    
    return metrics


def Scheme_eval(design, weight=None, draw=False):
    """è¯„ä¼°å·²è®­ç»ƒçš„æ¨¡å‹ - ä½¿ç”¨é‡å­ç‰¹å¾+OLS"""
    from sklearn.linear_model import LinearRegression
    from sklearn.metrics import r2_score
   
    
    result = {}
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # è·å–æ•°æ®åŠ è½½å™¨
    train_loader, test_loader = get_data_loaders(batch_size=32)
    arch_code = [6, 2]
    model = Cell(arch_code, design, 6, 1).to(device)
    
    # åŠ è½½é¢„è®­ç»ƒæƒé‡
    if weight is not None:
        model.load_state_dict(weight, strict=False)       
   
    train_features = []
    train_targets = []
    
    model.eval()
    with torch.no_grad():
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)
            
            # è·å–é‡å­ç‰¹å¾ï¼ˆä¸æ˜¯æœ€ç»ˆé¢„æµ‹ï¼‰
            features = model(data, return_features=True)
            
            train_features.append(features.detach().cpu().numpy())
            train_targets.append(target.detach().cpu().numpy())
    
    train_features = np.concatenate(train_features, axis=0)
    train_targets = np.concatenate(train_targets, axis=0).flatten()  
    
    # æå–æµ‹è¯•é›†ç‰¹å¾
    test_features = []
    test_targets = []
    
    with torch.no_grad():
        for batch_idx, (data, target) in enumerate(test_loader):
            data, target = data.to(device), target.to(device)
            
            # è·å–é‡å­ç‰¹å¾
            features = model(data, return_features=True)
            
            test_features.append(features.detach().cpu().numpy())
            test_targets.append(target.detach().cpu().numpy())
    
    test_features = np.concatenate(test_features, axis=0)
    test_targets = np.concatenate(test_targets, axis=0).flatten()
    
 
    
    # ç¬¬ä¸‰æ­¥ï¼šä½¿ç”¨æœ€å°äºŒä¹˜æ³•è®­ç»ƒçº¿æ€§å›å½’å™¨
    regressor = LinearRegression(fit_intercept=True)
    regressor.fit(train_features, train_targets)
    
    # è®¡ç®—è®­ç»ƒé›†æ€§èƒ½
    train_pred = regressor.predict(train_features)
    train_r2 = r2_score(train_targets, train_pred)
    train_nmse = nmse(train_targets, train_pred)
    train_capacity = compute_capacity(train_targets, train_pred)

    # ç¬¬å››æ­¥ï¼šæµ‹è¯•é›†è¯„ä¼°
    test_pred = regressor.predict(test_features)
    
    # è®¡ç®—å„ç§æŒ‡æ ‡
    test_r2 = r2_score(test_targets, test_pred)
    test_nmse = nmse(test_targets, test_pred)
    test_capacity = compute_capacity(test_targets, test_pred)
    
    print(f" Train Capacity: {train_capacity:.6f}")
    print(f" Test Capacity: {test_capacity:.6f}")

    
    # ç»„è£…ç»“æœ
    result = {
        'nmse': test_nmse,
        'capacity': test_capacity,
        'r2_score': test_r2,        
        'train_nmse': train_nmse,
        'train_capacity': train_capacity,
        'train_r2': train_r2,
        'regressor': regressor,
        'test_predictions': test_pred,
        'test_targets': test_targets,
        'train_features_shape': train_features.shape,
        'test_features_shape': test_features.shape
    }

     # å¯è§†åŒ–ç»“æœï¼ˆå¯é€‰ï¼‰
    if draw == True:
        try:
            import matplotlib.pyplot as plt
            
            fig, axes = plt.subplots(1, 2, figsize=(12, 5))
            
            # è®­ç»ƒé›†é¢„æµ‹ vs çœŸå€¼
            axes[0].scatter(train_targets, train_pred, alpha=0.6, s=10)
            axes[0].plot([train_targets.min(), train_targets.max()], 
                        [train_targets.min(), train_targets.max()], 'r--', lw=2)
            axes[0].set_xlabel('True Values')
            axes[0].set_ylabel('Predictions')
            axes[0].set_title(f'Training Set\nCapacity = {train_capacity:.4f}')
            axes[0].grid(True, alpha=0.3)
            
            # æµ‹è¯•é›†é¢„æµ‹ vs çœŸå€¼
            axes[1].scatter(test_targets, test_pred, alpha=0.6, s=10)
            axes[1].plot([test_targets.min(), test_targets.max()], 
                        [test_targets.min(), test_targets.max()], 'r--', lw=2)
            axes[1].set_xlabel('True Values')
            axes[1].set_ylabel('Predictions')
            axes[1].set_title(f'Test Set\nCapacity = {test_capacity:.4f}')
            axes[1].grid(True, alpha=0.3)
            
            plt.tight_layout()
            plot_file = f"figs/qrc_regression_plot_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
            plt.savefig(plot_file, dpi=300, bbox_inches='tight')
            plt.show(block=False)
            plt.pause(10)
            plt.close(fig)
            
            print(f"ğŸ“Š å›¾è¡¨å·²ä¿å­˜åˆ°: {plot_file}")
            
        except ImportError:
            print("âš ï¸ matplotlibæœªå®‰è£…ï¼Œè·³è¿‡å¯è§†åŒ–")
        
    return result


def check_vqc_gradients(model, threshold=1e-8):
    """æ£€æŸ¥VQCå‚æ•°çš„æ¢¯åº¦æƒ…å†µ"""
    vqc_grad_norms = []
    total_params = 0
    zero_grad_params = 0
    
    for name, param in model.named_parameters():
        if param.grad is not None:
            grad_norm = param.grad.norm().item()
            vqc_grad_norms.append(grad_norm)
            total_params += 1
            
            if grad_norm < threshold:
                zero_grad_params += 1
                
            # æ‰“å°å…³é”®å±‚çš„æ¢¯åº¦
            if 'quantum' in name.lower() or 'vqc' in name.lower():
                print(f"  {name}: grad_norm = {grad_norm:.8f}")
    
    if zero_grad_params > 0:
        print(f"WARNING: {zero_grad_params}/{total_params} parameters have near-zero gradients")
    
    return vqc_grad_norms


def check_parameter_updates(model, epoch_interval=10):
    """æ£€æŸ¥å‚æ•°æ˜¯å¦åœ¨æ›´æ–°"""
    if not hasattr(check_parameter_updates, 'prev_params'):
        # å­˜å‚¨å‰ä¸€æ¬¡çš„å‚æ•°
        check_parameter_updates.prev_params = {}
        check_parameter_updates.call_count = 0
        
    check_parameter_updates.call_count += 1
    
    if check_parameter_updates.call_count % epoch_interval == 0:
        for name, param in model.named_parameters():
            if name in check_parameter_updates.prev_params:
                param_diff = torch.norm(param.data - check_parameter_updates.prev_params[name]).item()
                if 'quantum' in name.lower() or 'vqc' in name.lower():
                    print(f"  {name}: param_change = {param_diff:.8f}")
            
            check_parameter_updates.prev_params[name] = param.data.clone()

def diagnose_gradient_explosion(model, data_loader, device):
    """è¯Šæ–­æ¢¯åº¦çˆ†ç‚¸çš„åŸå› """
    model.train()
    
    # æ£€æŸ¥ä¸€ä¸ªbatchçš„è¯¦ç»†æƒ…å†µ
    for data, target in data_loader:
        data, target = data.to(device), target.to(device)
        
        print("=== Gradient Explosion Diagnosis ===")
        print(f"Input data range: [{data.min():.6f}, {data.max():.6f}]")
        print(f"Target range: [{target.min():.6f}, {target.max():.6f}]")
        
        # æ£€æŸ¥åˆå§‹å‚æ•°
        print("\n--- Initial Parameters ---")
        for name, param in model.named_parameters():
            print(f"{name}: range=[{param.min():.6f}, {param.max():.6f}], norm={param.norm():.6f}")
        
        # å‰å‘ä¼ æ’­
        output = model(data)
        print(f"\nModel output range: [{output.min():.6f}, {output.max():.6f}]")
        print(f"Output norm: {output.norm():.6f}")
        
        # è®¡ç®—æŸå¤±
        loss = nn.MSELoss()(output.squeeze(-1), target)
        print(f"Loss: {loss.item():.6f}")
        
        # åå‘ä¼ æ’­
        loss.backward()
        
        # æ£€æŸ¥æ¢¯åº¦
        print("\n--- Gradients ---")
        for name, param in model.named_parameters():
            if param.grad is not None:
                grad_norm = param.grad.norm().item()
                grad_max = param.grad.max().item()
                grad_min = param.grad.min().item()
                print(f"{name}: norm={grad_norm:.6f}, range=[{grad_min:.6f}, {grad_max:.6f}]")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸å¤§çš„æ¢¯åº¦
                if grad_norm > 1000:
                    print(f"  âš ï¸  LARGE GRADIENT in {name}")
                    print(f"  Sample gradient values: {param.grad.flatten()[:5]}")
        
        break  # åªæ£€æŸ¥ç¬¬ä¸€ä¸ªbatch

def analyze_model_output_diversity(model, data_loader, device, sample_size=100):
    """åˆ†ææ¨¡å‹è¾“å‡ºçš„å¤šæ ·æ€§"""
    model.eval()
    outputs = []
    
    sample_count = 0
    with torch.no_grad():
        for data, _ in data_loader:
            if sample_count >= sample_size:
                break
                
            data = data.to(device)
            output = model(data)
            outputs.append(output.detach().cpu().numpy())
            sample_count += len(data)
    
    outputs = np.concatenate(outputs)[:sample_size]
    
    print(f"\n=== Model Output Analysis (n={len(outputs)}) ===")
    print(f"Min: {outputs.min():.6f}, Max: {outputs.max():.6f}")
    print(f"Mean: {outputs.mean():.6f}, Std: {outputs.std():.6f}")
    print(f"Unique values (rounded to 6 decimals): {len(np.unique(np.round(outputs, 6)))}")
    print(f"Range: {outputs.max() - outputs.min():.6f}")
    
    # æ£€æŸ¥æ˜¯å¦æ¥è¿‘å¸¸æ•°é¢„æµ‹
    if outputs.std() < 1e-6:
        print("âš ï¸  WARNING: Model output has very low variance (near-constant prediction)")
        return False
    else:
        print("âœ… Model output shows reasonable diversity")
        return True


def enhanced_train(model, train_loader, optimizer, criterion, device, epoch):
    """å¢å¼ºç‰ˆè®­ç»ƒå‡½æ•°ï¼ŒåŒ…å«è¯¦ç»†ç›‘æ§"""
    model.train()
    running_loss = 0.0
    all_preds = []
    all_targets = []

    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)

        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output.squeeze(-1), target)
        loss.backward()

        # æ¯10ä¸ªepochæ£€æŸ¥ä¸€æ¬¡æ¢¯åº¦
        if epoch % 10 == 0 and batch_idx == 0:
            print(f"\n--- Epoch {epoch} Gradient Check ---")
            grad_norms = check_vqc_gradients(model)
            
            if len(grad_norms) > 0:
                avg_grad = np.mean(grad_norms)
                print(f"Average gradient norm: {avg_grad:.8f}")
                
                if avg_grad < 1e-8:
                    print("âš ï¸  WARNING: Very small gradients detected!")

        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        optimizer.step()

        running_loss += loss.item()
        all_preds.append(output.detach().cpu().numpy())
        all_targets.append(target.detach().cpu().numpy())

    # æ£€æŸ¥è¾“å‡ºå¤šæ ·æ€§
    if epoch % 20 == 0:
        analyze_model_output_diversity(model, train_loader, device)

    # è®¡ç®—æŒ‡æ ‡
    all_preds = np.concatenate(all_preds)
    all_targets = np.concatenate(all_targets)
    train_nmse = nmse(all_targets, all_preds)
    train_capacity = compute_capacity(all_targets, all_preds)

    return running_loss / len(train_loader), train_nmse, train_capacity

def Scheme_enhanced(arch_code, design, weight='init', epochs=None, verbs=None, save=None):

    """å¢å¼ºç‰ˆè®­ç»ƒå‡½æ•°ï¼Œä¸“é—¨ç”¨äºè¯Šæ–­VQCè®­ç»ƒé—®é¢˜"""
    seed = 42
    set_seed(seed)
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    if epochs is None:
        epochs = 100
    
    train_loader, test_loader = get_data_loaders()
    
    model = Cell(arch_code, design, 6, 1).to(device)
    
    # æ‰“å°æ¨¡å‹ç»“æ„
    print("=== Model Architecture ===")
    for name, param in model.named_parameters():
        print(f"{name}: {param.shape}, requires_grad: {param.requires_grad}")
    
    if weight != 'init':
        if weight != 'base':
            model.load_state_dict(weight, strict=False)
        else:
            model.load_state_dict(torch.load('init_weights/base_qrc'), strict=False)

    # # è¯Šæ–­æ¢¯åº¦çˆ†ç‚¸
    # print("=== Diagnosing Gradient Explosion ===")
    # diagnose_gradient_explosion(model, train_loader, device)

    # åˆ†æåˆå§‹è¾“å‡º
    print("\n=== Initial Model Output ===")
    analyze_model_output_diversity(model, train_loader, device)
    
    criterion = nn.MSELoss()
    
    # å°è¯•ä¸åŒçš„å­¦ä¹ ç‡å’Œä¼˜åŒ–å™¨
    # optimizer = optim.Adam(model.parameters(), lr=0.01)  # å¢å¤§å­¦ä¹ ç‡
    # æˆ–è€…å°è¯• SGD
    optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)
    
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=5)
    
    train_losses = []
    test_losses = []
    train_nmses = []
    test_nmses = []
    train_capacitys = []
    test_capacitys = []
    best_capacity = 0

    start = time.time()
    for epoch in range(epochs):
        # ä½¿ç”¨å¢å¼ºç‰ˆè®­ç»ƒå‡½æ•°
        train_loss, train_nmse, train_capacity = enhanced_train(model, train_loader, optimizer, criterion, device, epoch)
            
        train_losses.append(train_loss)
        train_nmses.append(train_nmse)
        train_capacitys.append(train_capacity)
        
        test_loss, test_nmse, test_capacity, test_preds, test_targets = test(model, test_loader, criterion, device)
        test_nmses.append(test_nmse)
        test_losses.append(test_loss)
        test_capacitys.append(test_capacity)

        scheduler.step(train_loss)
        
        combined_score = train_capacity

        if combined_score > best_capacity:
            best_capacity = combined_score
            if not verbs:
                print(f'Epoch [{epoch + 1}/{epochs}], LR: {optimizer.param_groups[0]["lr"]:.6f}, Train Loss: {train_loss:.6f}, Train NMSE: {train_nmse:.6f}, Train Capacity: {train_capacity:.6f}, Test NMSE: {test_nmse:.6f}, Test Capacity: {test_capacity:.6f}, saving model')
            best_model = copy.deepcopy(model)
        else:
            if not verbs:
                print(f'Epoch [{epoch + 1}/{epochs}], LR: {optimizer.param_groups[0]["lr"]:.6f}, Train Loss: {train_loss:.6f}, Train NMSE: {train_nmse:.6f}, Train Capacity: {train_capacity:.6f}, Test NMSE: {test_nmse:.6f}, Test Capacity: {test_capacity:.6f}')

    end = time.time()
    
    # æœ€ç»ˆåˆ†æ
    print("\n=== Final Model Output ===")
    analyze_model_output_diversity(best_model, test_loader, device)
    
    metrics = evaluate(best_model, test_loader, device)
    display(metrics)
    print("Running time: %s seconds" % (end - start))
    
    report = {
        'train_loss_list': train_losses,
        'test_loss_list': test_losses,
        'train_nmse_list': train_nmses,
        'test_nmse_list': test_nmses,
        'train_capacity_list': train_capacitys,
        'test_capacity_list': test_capacitys,
        'best_capacity': best_capacity,
        'metrics': metrics
    }
    
    if save:
        torch.save(best_model.state_dict(), 'init_weights/init_stock_qrc')
    
    return best_model, report

class QuantumEscapeScheduler:
    """é‡å­æ¨¡å‹ä¸“ç”¨çš„é€ƒé€¸è°ƒåº¦å™¨"""
    
    def __init__(self, optimizer, base_lr=0.001, escape_factor=5.0, 
                 explosion_threshold=1000.0, patience=3, cooldown=5):
        self.optimizer = optimizer
        self.base_lr = base_lr
        self.escape_factor = escape_factor  # é€ƒé€¸æ—¶å­¦ä¹ ç‡æ”¾å¤§å€æ•°
        self.explosion_threshold = explosion_threshold  # æ¢¯åº¦çˆ†ç‚¸é˜ˆå€¼
        self.patience = patience  # è¿ç»­çˆ†ç‚¸å¤šå°‘æ¬¡è§¦å‘é€ƒé€¸
        self.cooldown = cooldown  # é€ƒé€¸åçš„å†·å´æœŸ
        
        # çŠ¶æ€è·Ÿè¸ª
        self.explosion_count = 0
        self.escape_mode = False
        self.cooldown_counter = 0
        self.last_grad_norm = 0
        self.last_loss = 0
        
        # å†å²è®°å½•
        self.grad_history = []
        self.loss_history = []
        
    def step(self, grad_norm, loss_value):
        """æ ¹æ®æ¢¯åº¦èŒƒæ•°å’ŒæŸå¤±å€¼è°ƒæ•´å­¦ä¹ ç‡"""
        self.grad_history.append(grad_norm)
        self.loss_history.append(loss_value)
        
        # ä¿æŒæœ€è¿‘10æ¬¡çš„å†å²
        if len(self.grad_history) > 10:
            self.grad_history.pop(0)
            self.loss_history.pop(0)
        
        current_lr = self.optimizer.param_groups[0]['lr']
        
        # æ£€æµ‹æ¢¯åº¦çˆ†ç‚¸
        explosion_detected = False
        
        # æ–¹å¼1ï¼šç»å¯¹é˜ˆå€¼æ£€æµ‹
        if grad_norm > self.explosion_threshold:
            explosion_detected = True
            print(f"ğŸ”¥ Gradient explosion detected: {grad_norm:.2f} > {self.explosion_threshold}")
        
        # æ–¹å¼2ï¼šç›¸å¯¹å¢é•¿æ£€æµ‹
        if len(self.grad_history) >= 2:
            recent_avg = np.mean(self.grad_history[-3:-1]) if len(self.grad_history) >= 3 else self.grad_history[-2]
            if grad_norm > 10 * recent_avg and grad_norm > 100:
                explosion_detected = True
                print(f"ğŸ“ˆ Rapid gradient increase detected: {grad_norm:.2f} vs recent avg {recent_avg:.2f}")
        
        # æ–¹å¼3ï¼šæŸå¤±çªç„¶å¢å¤§
        if len(self.loss_history) >= 2:
            prev_loss = self.loss_history[-2]
            if loss_value > 5 * prev_loss and loss_value > 1.0:
                explosion_detected = True
                print(f"ğŸ’¥ Loss explosion detected: {loss_value:.6f} vs {prev_loss:.6f}")
        
        if explosion_detected:
            self.explosion_count += 1
            print(f"çˆ†ç‚¸è®¡æ•°: {self.explosion_count}/{self.patience}")
        else:
            self.explosion_count = max(0, self.explosion_count - 1)  # ç¼“æ…¢è¡°å‡
        
        # å†³å®šæ˜¯å¦è¿›å…¥é€ƒé€¸æ¨¡å¼
        if self.explosion_count >= self.patience and not self.escape_mode:
            self.escape_mode = True
            self.cooldown_counter = 0
            new_lr = self.base_lr * self.escape_factor
            
            print(f"ğŸš€ ESCAPE MODE ACTIVATED!")
            print(f"   å­¦ä¹ ç‡ä» {current_lr:.8f} æå‡åˆ° {new_lr:.8f}")
            print(f"   é€ƒé€¸å€æ•°: {self.escape_factor}x")
            
            for param_group in self.optimizer.param_groups:
                param_group['lr'] = new_lr
                
        elif self.escape_mode:
            # é€ƒé€¸æ¨¡å¼ä¸­ï¼Œé€æ¸å†·å´
            self.cooldown_counter += 1
            
            if self.cooldown_counter >= self.cooldown:
                # æ£€æŸ¥æ˜¯å¦å·²ç»é€ƒç¦»é™·é˜±
                recent_grad_avg = np.mean(self.grad_history[-3:]) if len(self.grad_history) >= 3 else grad_norm
                
                if recent_grad_avg < self.explosion_threshold / 2:  # æ¢¯åº¦å·²ç»ç¨³å®š
                    self.escape_mode = False
                    self.explosion_count = 0
                    new_lr = self.base_lr
                    
                    print(f"âœ… ESCAPE SUCCESSFUL! æ¢¯åº¦å·²ç¨³å®š: {recent_grad_avg:.2f}")
                    print(f"   å­¦ä¹ ç‡æ¢å¤åˆ°åŸºç¡€å€¼: {new_lr:.8f}")
                    
                    for param_group in self.optimizer.param_groups:
                        param_group['lr'] = new_lr
                else:
                    print(f"ğŸ”„ ç»§ç»­é€ƒé€¸ä¸­... å½“å‰æ¢¯åº¦: {recent_grad_avg:.2f}")
        
        # æ­£å¸¸æƒ…å†µä¸‹çš„æ¸©å’Œè°ƒæ•´ï¼ˆç±»ä¼¼ä¼ ç»Ÿè°ƒåº¦å™¨ï¼‰
        elif not self.escape_mode and len(self.loss_history) >= 5:
            recent_losses = self.loss_history[-5:]
            if all(recent_losses[i] >= recent_losses[i+1] for i in range(len(recent_losses)-1)):
                # æŸå¤±æŒç»­ä¸‹é™ï¼Œå¯ä»¥ç¨å¾®æé«˜å­¦ä¹ ç‡
                new_lr = min(current_lr * 1.05, self.base_lr * 2)
                if new_lr != current_lr:
                    for param_group in self.optimizer.param_groups:
                        param_group['lr'] = new_lr
                    print(f"ğŸ“Š æ€§èƒ½è‰¯å¥½ï¼Œå­¦ä¹ ç‡å°å¹…æå‡: {current_lr:.8f} -> {new_lr:.8f}")
        
        self.last_grad_norm = grad_norm
        self.last_loss = loss_value
        
        return self.escape_mode


def quantum_aware_train(model, train_loader, optimizer, criterion, device, epoch, scheduler):
    """é‡å­æ„ŸçŸ¥çš„è®­ç»ƒå‡½æ•°"""
    model.train()
    running_loss = 0.0
    all_preds = []
    all_targets = []
    
    epoch_max_grad = 0
    batch_count = 0

    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)

        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output.squeeze(-1), target)
        
        # æ£€æŸ¥losså¼‚å¸¸
        if torch.isnan(loss) or torch.isinf(loss):
            print(f"âš ï¸  NaN/Inf loss detected, skipping batch")
            continue
            
        loss.backward()

        # è®¡ç®—æ¢¯åº¦èŒƒæ•°ï¼ˆåˆ†åˆ«è®¡ç®—é‡å­å’Œç»å…¸å‚æ•°ï¼‰
        quantum_grad_norm = 0
        classical_grad_norm = 0
        total_grad_norm = 0
        
        for name, param in model.named_parameters():
            if param.grad is not None:
                param_norm = param.grad.data.norm(2).item()
                total_grad_norm += param_norm ** 2
                
                if 'q_params' in name or 'g_raw' in name:
                    quantum_grad_norm += param_norm ** 2
                else:
                    classical_grad_norm += param_norm ** 2
        
        total_grad_norm = total_grad_norm ** 0.5
        quantum_grad_norm = quantum_grad_norm ** 0.5
        classical_grad_norm = classical_grad_norm ** 0.5
        
        epoch_max_grad = max(epoch_max_grad, quantum_grad_norm)
        
        # ä½¿ç”¨é‡å­è°ƒåº¦å™¨
        escape_mode = scheduler.step(quantum_grad_norm, loss.item())
        
        # è‡ªé€‚åº”æ¢¯åº¦å¤„ç†
        if escape_mode:
            # é€ƒé€¸æ¨¡å¼ï¼šä½¿ç”¨è¾ƒå¤§çš„æ¢¯åº¦è£å‰ªé˜ˆå€¼ï¼Œå…è®¸å¤§æ­¥é•¿
            if epoch > 10 and quantum_grad_norm > 50000:  # åªåœ¨epoch>10åæ‰è·³è¿‡æç«¯æ¢¯åº¦
                print(f"ğŸ’€ æç«¯æ¢¯åº¦ {quantum_grad_norm:.2f}ï¼Œè·³è¿‡æ­¤batch")
                optimizer.zero_grad()
                continue
            else:
                # åœ¨é€ƒé€¸æ¨¡å¼ä¸‹ä½¿ç”¨æ›´å¤§çš„è£å‰ªé˜ˆå€¼
                if quantum_grad_norm > 50000:  # è®­ç»ƒåˆæœŸä¸è·³è¿‡ï¼Œä½†ä½¿ç”¨å¼ºåŠ›è£å‰ª
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=100.0)
                    print(f"ğŸš€ é€ƒé€¸æ¨¡å¼(åˆæœŸ): æå¤§æ¢¯åº¦={quantum_grad_norm:.2f}, ä½¿ç”¨è¶…å¼ºè£å‰ª")
                else:
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=50.0)
                    print(f"ğŸš€ é€ƒé€¸æ¨¡å¼: æ¢¯åº¦={quantum_grad_norm:.2f}, ä½¿ç”¨å¤§è£å‰ªé˜ˆå€¼")
        else:
            # æ­£å¸¸æ¨¡å¼ï¼šæ ‡å‡†æ¢¯åº¦è£å‰ª
            if epoch > 10 and quantum_grad_norm > 10000:  # åªåœ¨epoch>10åæ‰è·³è¿‡å¤§æ¢¯åº¦
                print(f"âš ï¸  å¤§æ¢¯åº¦ {quantum_grad_norm:.2f}ï¼Œè·³è¿‡æ­¤batch")
                optimizer.zero_grad()
                continue
            else:
                # è®­ç»ƒåˆæœŸä¸è·³è¿‡ï¼Œä½†ä½¿ç”¨é€‚å½“çš„è£å‰ª
                if quantum_grad_norm > 10000:
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)
                    print(f"âš ï¸  æ­£å¸¸æ¨¡å¼(åˆæœŸ): å¤§æ¢¯åº¦={quantum_grad_norm:.2f}, ä½¿ç”¨å¼ºè£å‰ª")
                else:
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        optimizer.step()
        batch_count += 1

        running_loss += loss.item()
        all_preds.append(output.detach().cpu().numpy())
        all_targets.append(target.detach().cpu().numpy())

    # è®¡ç®—æŒ‡æ ‡
    if len(all_preds) > 0:
        all_preds = np.concatenate(all_preds)
        all_targets = np.concatenate(all_targets)
        train_nmse = nmse(all_targets, all_preds)
        train_capacity = compute_capacity(all_targets, all_preds)
    else:
        train_nmse = float('inf')
        train_capacity = 0.0
    
    print(f"Epoch {epoch}: æœ€å¤§æ¢¯åº¦={epoch_max_grad:.2f}, æœ‰æ•ˆbatch={batch_count}")

    return running_loss / max(1, len(all_preds)), train_nmse, train_capacity


def Scheme_Quantum_Escape(arch_code, design, weight='init', epochs=None, verbs=None, save=None):
    """ä½¿ç”¨é‡å­é€ƒé€¸è°ƒåº¦å™¨çš„è®­ç»ƒæ–¹æ¡ˆ"""
    seed = 42
    set_seed(seed)
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    if epochs is None:
        epochs = 100
    
    train_loader, test_loader = get_data_loaders(batch_size=16)
    
    model = Cell(arch_code, design, 6, 1).to(device)
    
    print("=== Model Architecture ===")
    for name, param in model.named_parameters():
        print(f"{name}: {param.shape}, requires_grad: {param.requires_grad}")
    
    if weight != 'init':
        if weight != 'base':
            model.load_state_dict(weight, strict=False)
        else:
            model.load_state_dict(torch.load('init_weights/base_qrc'), strict=False)
    
    criterion = nn.MSELoss()
    
    # ä½¿ç”¨ä¸­ç­‰å­¦ä¹ ç‡ä½œä¸ºåŸºç¡€
    base_lr = 0.01
    optimizer = optim.Adam(model.parameters(), lr=base_lr)
    
    # åˆ›å»ºé‡å­é€ƒé€¸è°ƒåº¦å™¨
    quantum_scheduler = QuantumEscapeScheduler(
        optimizer, 
        base_lr=base_lr,
        escape_factor=3.0,  # é€ƒé€¸æ—¶å­¦ä¹ ç‡æ”¾å¤§3å€
        explosion_threshold=1000.0,
        patience=2,  # è¿ç»­2æ¬¡çˆ†ç‚¸å°±è§¦å‘é€ƒé€¸
        cooldown=3   # é€ƒé€¸3ä¸ªepochåæ£€æŸ¥æ˜¯å¦æˆåŠŸ
    )
    
    train_losses = []
    test_losses = []
    train_nmses = []
    test_nmses = []
    train_capacitys = []
    test_capacitys = []
    best_capacity = 0
    
    start = time.time()
    for epoch in range(epochs):
        print(f"\n--- Epoch {epoch+1} (Quantum Escape Training) ---")
        
        # ä½¿ç”¨é‡å­æ„ŸçŸ¥è®­ç»ƒ
        train_loss, train_nmse, train_capacity = quantum_aware_train(
            model, train_loader, optimizer, criterion, device, epoch, quantum_scheduler
        )
        
        train_losses.append(train_loss)
        train_nmses.append(train_nmse)
        train_capacitys.append(train_capacity)
        
        test_loss, test_nmse, test_capacity, test_preds, test_targets = test(
            model, test_loader, criterion, device
        )
        test_nmses.append(test_nmse)
        test_losses.append(test_loss)
        test_capacitys.append(test_capacity)
        
        combined_score = train_capacity

        if combined_score > best_capacity:
            best_capacity = combined_score
            best_model = copy.deepcopy(model)
            print(f'Epoch [{epoch + 1}/{epochs}], LR: {optimizer.param_groups[0]["lr"]:.6f}, Train Loss: {train_loss:.6f}, Train NMSE: {train_nmse:.6f}, Train Capacity: {train_capacity:.6f}, Test NMSE: {test_nmse:.6f}, Test Capacity: {test_capacity:.6f}, saving model')
        else:
            print(f'Epoch [{epoch + 1}/{epochs}], LR: {optimizer.param_groups[0]["lr"]:.6f}, Train Loss: {train_loss:.6f}, Train NMSE: {train_nmse:.6f}, Train Capacity: {train_capacity:.6f}, Test NMSE: {test_nmse:.6f}, Test Capacity: {test_capacity:.6f}')

    end = time.time()
    
    metrics = evaluate(best_model, test_loader, device)
    display(metrics)
    print("Running time: %s seconds" % (end - start))
    
    return best_model, {
        'train_loss_list': train_losses,
        'test_loss_list': test_losses,
        'train_nmse_list': train_nmses,
        'test_nmse_list': test_nmses,
        'train_capacity_list': train_capacitys,
        'test_capacity_list': test_capacitys,
        'best_capacity': best_capacity,
        'nmse': metrics
    }


if __name__ == '__main__':
    single = [[i] + [1] * 1 * n_layers for i in range(1, n_qubits + 1)]
    enta = [[i] + [i + 1] * n_layers for i in range(1, n_qubits)] + [[n_qubits] + [1] * n_layers]

    arch_code = [6, 2]
    design = single_enta_to_design(single, enta, (n_qubits, n_layers))
    
    # ä½¿ç”¨é‡å­é€ƒé€¸è°ƒåº¦å™¨
    print("å¼€å§‹é‡å­é€ƒé€¸è®­ç»ƒ...")
    # best_model, report = Scheme_Quantum_Escape(arch_code, design, 'init', 100)
    best_model, report = Scheme_enhanced(arch_code, design, 'init', 200, None, 'save')

    result = Scheme_eval(design, weight=best_model.state_dict())

    # # ä¿å­˜æœ€ä½³æ¨¡å‹
    # torch.save(best_model.state_dict(), 'quantum_escape_model.pth')
    

   
