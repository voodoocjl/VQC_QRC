#!/usr/bin/env python3
"""
ä¸¤é‡å­æ¯”ç‰¹å¯†åº¦çŸ©é˜µVQCè®­ç»ƒæ¼”ç¤º
å±•ç¤ºå¦‚ä½•åˆ›å»ºå¯è®­ç»ƒçš„å˜åˆ†é‡å­ç”µè·¯å¹¶è¿›è¡Œåå‘ä¼ æ’­
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.utils.data
import sys
import os
import numpy as np

# æ·»åŠ è·¯å¾„ä»¥ä¾¿å¯¼å…¥torchquantum
sys.path.append(os.path.dirname(os.path.dirname(__file__)))

import torchquantum as tq


class DensityVQC(nn.Module):
    """ä¸¤é‡å­æ¯”ç‰¹å¯†åº¦çŸ©é˜µå˜åˆ†é‡å­ç”µè·¯"""
    
    def __init__(self, n_wires=2, n_layers=3):
        super().__init__()
        self.n_wires = n_wires
        self.n_layers = n_layers
        
        # å¯è®­ç»ƒå‚æ•° - æ¯å±‚åŒ…å«RYå’ŒRZæ—‹è½¬é—¨å‚æ•°
        self.ry_params = nn.Parameter(torch.randn(n_layers, n_wires) * 0.1)
        self.rz_params = nn.Parameter(torch.randn(n_layers, n_wires) * 0.1)
        
        print(f"åˆå§‹åŒ–VQC: {n_layers}å±‚, {n_wires}é‡å­æ¯”ç‰¹")
        print(f"RYå‚æ•°å½¢çŠ¶: {self.ry_params.shape}")
        print(f"RZå‚æ•°å½¢çŠ¶: {self.rz_params.shape}")
        print(f"æ€»å‚æ•°æ•°é‡: {self.ry_params.numel() + self.rz_params.numel()}")
    
    def forward(self, bsz=1, input_states=None):
        """å‰å‘ä¼ æ’­æ„å»ºVQC
        Args:
            bsz: æ‰¹æ¬¡å¤§å°
            input_states: è¾“å…¥é‡å­æ€ï¼Œå¦‚æœæä¾›åˆ™ç”¨äºåˆå§‹åŒ–é‡å­è®¾å¤‡
        """
        # åˆ›å»ºå¯†åº¦çŸ©é˜µè®¾å¤‡
        device = tq.NoiseDevice(n_wires=self.n_wires, bsz=bsz)
        
        # å¦‚æœæä¾›äº†è¾“å…¥æ€ï¼Œåˆ™å°†å…¶ç¼–ç åˆ°é‡å­è®¾å¤‡ä¸­
        if input_states is not None:
            # å°†è¾“å…¥æ€ç¼–ç ä¸ºå¯†åº¦çŸ©é˜µ
            batch_densities = []
            for i in range(bsz):
                if i < len(input_states):
                    input_state = input_states[i]
                    # åˆ›å»ºå¯†åº¦çŸ©é˜µ Ï = |ÏˆâŸ©âŸ¨Ïˆ|
                    rho = torch.outer(input_state.conj(), input_state)
                    # é‡å¡‘ä¸ºè®¾å¤‡æœŸæœ›çš„å½¢çŠ¶ [2, 2, 2, 2]
                    rho_reshaped = rho.reshape([2, 2, 2, 2])
                    batch_densities.append(rho_reshaped)
                else:
                    # å¦‚æœæ‰¹æ¬¡ä¸­æ²¡æœ‰è¶³å¤Ÿçš„è¾“å…¥æ€ï¼Œä½¿ç”¨é»˜è®¤åˆå§‹æ€
                    batch_densities.append(device.densities[0])
            
            # å †å æ‰€æœ‰å¯†åº¦çŸ©é˜µå¹¶è®¾ç½®åˆ°è®¾å¤‡
            if batch_densities:
                device.densities = torch.stack(batch_densities)
        
        # VQCå±‚çº§ç»“æ„
        for layer in range(self.n_layers):
            # 1. RYæ—‹è½¬é—¨å±‚
            for wire in range(self.n_wires):
                device.ry(wires=wire, params=self.ry_params[layer, wire])
            
            # 2. RZæ—‹è½¬é—¨å±‚  
            for wire in range(self.n_wires):
                device.rz(wires=wire, params=self.rz_params[layer, wire])
            
            # 3. çº ç¼ å±‚ - CNOTé—¨
            if self.n_wires > 1:
                for wire in range(self.n_wires - 1):
                    device.cnot(wires=[wire, wire + 1])
                # ç¯å½¢çº ç¼ ï¼ˆæœ€åä¸€ä¸ªå’Œç¬¬ä¸€ä¸ªç›¸è¿ï¼‰
                if self.n_wires > 2:
                    device.cnot(wires=[self.n_wires - 1, 0])
        
        return device
    
    def get_expectation_pauli_z(self, device, wire=0):
        """è®¡ç®—æŒ‡å®šé‡å­æ¯”ç‰¹çš„Pauli-ZæœŸæœ›å€¼"""
        # è·å–å¯†åº¦çŸ©é˜µå¹¶è½¬æ¢ä¸º2Då½¢å¼
        bsz = device.densities.shape[0]
        rho_2d = device.densities.reshape(bsz, 2**self.n_wires, 2**self.n_wires)
        
        # Pauli-Zç®—å­å¯¹åº”çš„å¯¹è§’çŸ©é˜µ (å¯¹2æ¯”ç‰¹ç³»ç»Ÿ)
        if self.n_wires == 2:
            if wire == 0:
                # ZâŠ—I: diag([1, 1, -1, -1])
                pauli_z = torch.diag(torch.tensor([1., 1., -1., -1.], 
                                                 dtype=torch.complex64, 
                                                 device=rho_2d.device))
            elif wire == 1:
                # IâŠ—Z: diag([1, -1, 1, -1])
                pauli_z = torch.diag(torch.tensor([1., -1., 1., -1.], 
                                                 dtype=torch.complex64, 
                                                 device=rho_2d.device))
        
        # è®¡ç®—æœŸæœ›å€¼ tr(Ï * Z) å¯¹æ¯ä¸ªæ‰¹æ¬¡
        batch_expectations = []
        for i in range(bsz):
            expectation = torch.real(torch.trace(rho_2d[i] @ pauli_z))
            batch_expectations.append(expectation)
        
        return torch.stack(batch_expectations)[0] if bsz == 1 else torch.stack(batch_expectations)
    
    def get_fidelity_with_target(self, device, target_state):
        """è®¡ç®—ä¸ç›®æ ‡æ€çš„ä¿çœŸåº¦"""
        bsz = device.densities.shape[0]
        rho = device.densities.reshape(bsz, 2**self.n_wires, 2**self.n_wires)
        
        # # ç›®æ ‡æ€çš„å¯†åº¦çŸ©é˜µ
        # target_rho = torch.outer(target_state.conj(), target_state)
        # target_rho = target_rho.to(rho.device)
        
        # å¯¹äºå•ä¸ªç›®æ ‡æ€ä¸æ‰¹æ¬¡å¯†åº¦çŸ©é˜µçš„ä¿çœŸåº¦è®¡ç®—
        batch_fidelities = []
        for i in range(bsz):
            target_rho = torch.outer(target_state[i].conj(), target_state[i])
            fidelity = torch.real(torch.trace(rho[i] @ target_rho))
            batch_fidelities.append(fidelity)
        
        if bsz == 1:
            return batch_fidelities[0]
        else:
            return torch.stack(batch_fidelities)


def print_tensor_info(tensor, name):
    """æ‰“å°å¼ é‡ä¿¡æ¯"""
    print(f"{name}:")
    print(f"  å€¼: {tensor.item() if tensor.numel() == 1 else tensor}")
    print(f"  æ¢¯åº¦: {tensor.grad}")
    print(f"  requires_grad: {tensor.requires_grad}")


def demo_vqc_training():
    """æ¼”ç¤ºVQCè®­ç»ƒè¿‡ç¨‹"""
    print("ğŸš€ å¯†åº¦çŸ©é˜µVQCè®­ç»ƒæ¼”ç¤º")
    print("=" * 60)
    
    # 1. åˆ›å»ºVQCæ¨¡å‹
    print("\n1. åˆ›å»ºVQCæ¨¡å‹")
    vqc = DensityVQC(n_wires=2, n_layers=3)
    
    # 2. ç”Ÿæˆéšæœºè®­ç»ƒæ•°æ®
    print("\n2. ç”Ÿæˆéšæœºè®­ç»ƒæ•°æ®")
    n_samples = 100
    batch_size = 16
    
    # ç”Ÿæˆéšæœºé‡å­æ€å¯¹ (X, Y)
    np.random.seed(42)  # å›ºå®šéšæœºç§å­ç¡®ä¿å¯é‡å¤æ€§
    torch.manual_seed(42)
    
    # ç”Ÿæˆè¾“å…¥é‡å­æ€ X (100ä¸ªä¸¤æ¯”ç‰¹é‡å­æ€)
    X_real = torch.randn(n_samples, 4) * 0.5
    X_imag = torch.randn(n_samples, 4) * 0.5
    X_states = torch.complex(X_real, X_imag)
    X_states = X_states / torch.norm(X_states, dim=1, keepdim=True)
    
    # ç”Ÿæˆè¾“å‡ºé‡å­æ€ Y (100ä¸ªä¸¤æ¯”ç‰¹é‡å­æ€)
    Y_real = torch.randn(n_samples, 4) * 0.5
    Y_imag = torch.randn(n_samples, 4) * 0.5
    Y_states = torch.complex(Y_real, Y_imag)
    Y_states = Y_states / torch.norm(Y_states, dim=1, keepdim=True)
    
    print(f"ç”Ÿæˆ {n_samples} å¯¹éšæœºé‡å­æ€ (X, Y)")
    print(f"æ‰¹å¤„ç†å¤§å°: {batch_size}")
    print(f"è¾“å…¥æ€Xå½¢çŠ¶: {X_states.shape}")
    print(f"è¾“å‡ºæ€Yå½¢çŠ¶: {Y_states.shape}")
    print(f"å‰3å¯¹é‡å­æ€ç¤ºä¾‹:")
    for i in range(3):
        print(f"  æ ·æœ¬ {i}:")
        print(f"    Xæ€: {X_states[i]}")
        print(f"    Yæ€: {Y_states[i]}")
        print(f"    Xæ¨¡é•¿: {torch.norm(X_states[i]).item():.6f}")
        print(f"    Yæ¨¡é•¿: {torch.norm(Y_states[i]).item():.6f}")
        print(f"    XÂ·Y*: {torch.abs(torch.vdot(X_states[i], Y_states[i])).item():.6f}")  # å†…ç§¯æ¨¡é•¿
    
    # 3. è®¾ç½®ä¼˜åŒ–å™¨
    print("\n3. è®¾ç½®ä¼˜åŒ–å™¨")
    optimizer = optim.Adam(vqc.parameters(), lr=0.01)  # é™ä½å­¦ä¹ ç‡é€‚åº”æ‰¹å¤„ç†
    print(f"ä¼˜åŒ–å™¨: Adam, å­¦ä¹ ç‡: 0.01")
    
    # 4. åˆ›å»ºæ•°æ®åŠ è½½å™¨
    dataset = torch.utils.data.TensorDataset(X_states, Y_states)
    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)
    print(f"æ•°æ®åŠ è½½å™¨: æ‰¹å¤§å°={batch_size}, æ€»æ‰¹æ¬¡={len(dataloader)}")
    print(f"æ¯ä¸ªæ‰¹æ¬¡åŒ…å«: {batch_size}å¯¹(X,Y)é‡å­æ€")
    
    # 5. è®­ç»ƒå¾ªç¯
    print("\n5. å¼€å§‹è®­ç»ƒ")
    print("-" * 50)
    
    n_epochs = 20
    all_losses = []
    epoch_losses = []
    
    for epoch in range(n_epochs):
        epoch_loss = 0.0
        batch_count = 0
        
        for batch_idx, (batch_X, batch_Y) in enumerate(dataloader):
            current_batch_size = batch_X.size(0)
            
            optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­ - ä½¿ç”¨è¾“å…¥æ€Xåˆå§‹åŒ–ï¼Œç„¶åç»è¿‡VQCå˜æ¢
            device = vqc.forward(bsz=current_batch_size, input_states=batch_X)
            
            # è®¡ç®—æ‰¹æ¬¡æŸå¤± - VQCè¾“å‡ºä¸ç›®æ ‡æ€Yçš„ä¿çœŸåº¦
                       
            batch_fidelities = vqc.get_fidelity_with_target(device, batch_Y)
                
            
            # å¹³å‡ä¿çœŸåº¦å’ŒæŸå¤±
            avg_fidelity = batch_fidelities.mean()
            loss = 1 - avg_fidelity
            
            # åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()
            
            epoch_loss += loss.item()
            batch_count += 1
            
            # æ¯5ä¸ªæ‰¹æ¬¡æ‰“å°ä¸€æ¬¡
            if batch_idx % 5 == 0:
                print(f"  Epoch {epoch:2d}, Batch {batch_idx:2d}: "
                      f"Loss={loss.item():.6f}, Fidelity={avg_fidelity.item():.6f}")
                if batch_idx == 0:  # æ˜¾ç¤ºç¬¬ä¸€ä¸ªæ‰¹æ¬¡çš„æ ·ä¾‹
                    print(f"    æ ·ä¾‹: Xâ†’Y æ˜ å°„ä¿çœŸåº¦: {batch_fidelities[0].item():.6f}")
        
        # è®°å½•epochå¹³å‡æŸå¤±
        avg_epoch_loss = epoch_loss / batch_count
        epoch_losses.append(avg_epoch_loss)
        all_losses.extend([avg_epoch_loss] * batch_count)
        
        print(f"Epoch {epoch:2d} å®Œæˆ: å¹³å‡æŸå¤±={avg_epoch_loss:.6f}")
    
    print("-" * 50)
    print(f"è®­ç»ƒå®Œæˆ! æœ€ç»ˆå¹³å‡æŸå¤±: {epoch_losses[-1]:.6f}")
    
    # 6. æµ‹è¯•é˜¶æ®µ - åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
    print("\n6. æµ‹è¯•é˜¶æ®µè¯„ä¼°")
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®å¯¹ï¼ˆä¸è®­ç»ƒæ•°æ®ä¸åŒï¼‰
    test_X_real = torch.randn(10, 4) * 0.3
    test_X_imag = torch.randn(10, 4) * 0.3
    test_X = torch.complex(test_X_real, test_X_imag)
    test_X = test_X / torch.norm(test_X, dim=1, keepdim=True)
    
    test_Y_real = torch.randn(10, 4) * 0.3
    test_Y_imag = torch.randn(10, 4) * 0.3
    test_Y = torch.complex(test_Y_real, test_Y_imag)
    test_Y = test_Y / torch.norm(test_Y, dim=1, keepdim=True)
    
    with torch.no_grad():
        test_fidelities = []
        
        # é€ä¸ªæµ‹è¯•æ¯å¯¹(X,Y)
        for i in range(10):
            # ç”¨è¾“å…¥æ€X[i]åˆå§‹åŒ–VQCï¼Œäº§ç”Ÿè¾“å‡º
            test_device = vqc.forward(bsz=1, input_states=[test_X[i]])
            
            # è®¡ç®—VQCè¾“å‡ºä¸ç›®æ ‡æ€Y[i]çš„ä¿çœŸåº¦
            fidelity = vqc.get_fidelity_with_target(test_device, test_Y)
            if isinstance(fidelity, torch.Tensor):
                fidelity = fidelity.item()
            test_fidelities.append(fidelity)
        
        avg_test_fidelity = np.mean(test_fidelities)
        print(f"æµ‹è¯•é›†å¹³å‡ä¿çœŸåº¦: {avg_test_fidelity:.6f}")
        print(f"æµ‹è¯•é›†ä¿çœŸåº¦æ ‡å‡†å·®: {np.std(test_fidelities):.6f}")
        print(f"æœ€ä½³æµ‹è¯•ä¿çœŸåº¦: {max(test_fidelities):.6f}")
        print(f"æœ€å·®æµ‹è¯•ä¿çœŸåº¦: {min(test_fidelities):.6f}")
        
        # æ˜¾ç¤ºå‡ ä¸ªå…·ä½“çš„æµ‹è¯•æ ·ä¾‹
        print(f"\nå…·ä½“æµ‹è¯•æ ·ä¾‹:")
        for i in range(3):
            print(f"  æ ·ä¾‹ {i}: Xâ†’Y ä¿çœŸåº¦ = {test_fidelities[i]:.6f}")
            print(f"    è¾“å…¥æ€æ¨¡é•¿: {torch.norm(test_X[i]).item():.6f}")
            print(f"    ç›®æ ‡æ€æ¨¡é•¿: {torch.norm(test_Y[i]).item():.6f}")
    
    # 7. åˆ†æä¸€ä¸ªå…·ä½“çš„æµ‹è¯•æ ·ä¾‹
    print("\n7. åˆ†æå…·ä½“æµ‹è¯•æ ·ä¾‹")
    with torch.no_grad():
        # é€‰æ‹©ç¬¬ä¸€ä¸ªæµ‹è¯•æ ·ä¾‹è¿›è¡Œè¯¦ç»†åˆ†æ
        sample_X = test_X[0]
        sample_Y = test_Y[0]
        
        sample_device = vqc.forward(bsz=1, input_states=[sample_X])
        sample_fidelity = vqc.get_fidelity_with_target(sample_device, sample_Y)
        
        # æ‰“å°ç›¸å…³å¯†åº¦çŸ©é˜µ
        final_rho = sample_device.densities[0].reshape(4, 4)
        input_rho = torch.outer(sample_X.conj(), sample_X)
        target_rho = torch.outer(sample_Y.conj(), sample_Y)
        
        print("è¾“å…¥å¯†åº¦çŸ©é˜µ Ï_X (å®éƒ¨):")
        print(input_rho.real.numpy())
        print("\nVQCè¾“å‡ºå¯†åº¦çŸ©é˜µ Ï_out (å®éƒ¨):")
        print(final_rho.real.numpy())
        print("\nç›®æ ‡å¯†åº¦çŸ©é˜µ Ï_Y (å®éƒ¨):")
        print(target_rho.real.numpy())
        print(f"\næ ·ä¾‹ä¿çœŸåº¦ F(Ï_out, Ï_Y): {sample_fidelity.item():.6f}")
        
        # è®¡ç®—è¾“å…¥ä¸ç›®æ ‡çš„ç›´æ¥ä¿çœŸåº¦ï¼ˆä½œä¸ºå‚è€ƒï¼‰
        direct_fidelity = torch.real(torch.trace(input_rho @ target_rho))
        print(f"ç›´æ¥ä¿çœŸåº¦ F(Ï_X, Ï_Y): {direct_fidelity.item():.6f}")
        print(f"VQCæ”¹å–„: {sample_fidelity.item() - direct_fidelity.item():.6f}")
        
        # æ£€æŸ¥VQCè¾“å‡ºå¯†åº¦çŸ©é˜µæ€§è´¨
        trace = torch.trace(final_rho)
        purity = torch.trace(final_rho @ final_rho)
        eigenvals = torch.linalg.eigvals(final_rho)
        min_eigenval = torch.min(eigenvals.real)
        
        print(f"\nVQCå¯†åº¦çŸ©é˜µæ€§è´¨:")
        print(f"è¿¹: {trace.real.item():.6f} (åº”â‰ˆ1)")
        print(f"çº¯åº¦: {purity.real.item():.6f} (çº¯æ€â‰ˆ1)")
        print(f"æœ€å°ç‰¹å¾å€¼: {min_eigenval.item():.6f} (åº”â‰¥0)")
        print(f"æ­£åŠå®š: {min_eigenval.item() >= -1e-6}")
    
    # 8. è®­ç»ƒç»Ÿè®¡
    print(f"\n8. è®­ç»ƒç»Ÿè®¡:")
    print(f"æ€»è®­ç»ƒæ ·æœ¬: {n_samples}")
    print(f"æ€»è®­ç»ƒæ‰¹æ¬¡: {len(dataloader) * n_epochs}")
    print(f"åˆå§‹å¹³å‡æŸå¤±: {epoch_losses[0]:.6f}")
    print(f"æœ€ç»ˆå¹³å‡æŸå¤±: {epoch_losses[-1]:.6f}")
    print(f"æŸå¤±æ”¹å–„: {epoch_losses[0] - epoch_losses[-1]:.6f}")
    
    return vqc, epoch_losses, test_fidelities


def test_gradients():
    """æµ‹è¯•æ¢¯åº¦è®¡ç®—"""
    print("\n" + "="*60)
    print("ğŸ”¬ æ¢¯åº¦è®¡ç®—æµ‹è¯•")
    print("="*60)
    
    vqc = DensityVQC(n_wires=2, n_layers=1)
    
    # ç®€å•çš„å‰å‘ä¼ æ’­
    device = vqc.forward(bsz=1)
    
    # è®¡ç®—ä¸€ä¸ªç®€å•çš„æŸå¤±å‡½æ•°ï¼šæœ€å°åŒ–ç¬¬ä¸€ä¸ªé‡å­æ¯”ç‰¹çš„ZæœŸæœ›å€¼
    z_expectation = vqc.get_expectation_pauli_z(device, wire=0)
    loss = z_expectation**2  # ç›®æ ‡ï¼šä½¿ZæœŸæœ›å€¼æ¥è¿‘0
    
    print(f"æŸå¤±å‡½æ•°: âŸ¨Zâ‚€âŸ©Â² = {loss.item():.6f}")
    
    # åå‘ä¼ æ’­
    loss.backward()
    
    # æ£€æŸ¥æ¢¯åº¦
    print("\næ¢¯åº¦æ£€æŸ¥:")
    if vqc.ry_params.grad is not None:
        print(f"RYå‚æ•°æ¢¯åº¦èŒƒæ•°: {torch.norm(vqc.ry_params.grad).item():.6f}")
        print(f"RYå‚æ•°æ¢¯åº¦: {vqc.ry_params.grad.flatten()}")
    else:
        print("RYå‚æ•°æ¢¯åº¦ä¸ºNone!")
        
    if vqc.rz_params.grad is not None:
        print(f"RZå‚æ•°æ¢¯åº¦èŒƒæ•°: {torch.norm(vqc.rz_params.grad).item():.6f}")
        print(f"RZå‚æ•°æ¢¯åº¦: {vqc.rz_params.grad.flatten()}")
    else:
        print("RZå‚æ•°æ¢¯åº¦ä¸ºNone!")


def main():
    """ä¸»å‡½æ•°"""
    # è¿è¡Œæ¢¯åº¦æµ‹è¯•
    test_gradients()
    
    # è¿è¡ŒVQCè®­ç»ƒæ¼”ç¤º
    vqc, epoch_losses, test_fidelities = demo_vqc_training()
    
    print("\n" + "="*60)
    print("ğŸ‰ æ¼”ç¤ºå®Œæˆ!")
    print("="*60)
    print("âœ… æˆåŠŸéªŒè¯:")
    print("  - å¯†åº¦çŸ©é˜µVQCæ„å»º")
    print("  - éšæœºæ•°æ®ç”Ÿæˆ (100ä¸ªæ ·æœ¬)")
    print("  - æ‰¹å¤„ç†è®­ç»ƒ (batch_size=16)")
    print("  - å¯è®­ç»ƒå‚æ•°å®šä¹‰")
    print("  - åå‘ä¼ æ’­è®¡ç®—")
    print("  - æ¢¯åº¦ä¸‹é™ä¼˜åŒ–")
    print("  - ä¿çœŸåº¦æŸå¤±å‡½æ•°")
    print("  - æœŸæœ›å€¼è®¡ç®—")
    print("  - æµ‹è¯•é›†è¯„ä¼°")
    
    print(f"\nğŸ“Š æœ€ç»ˆç»“æœ:")
    print(f"  è®­ç»ƒæŸå¤±æ”¹å–„: {epoch_losses[0] - epoch_losses[-1]:.6f}")
    print(f"  æµ‹è¯•é›†å¹³å‡ä¿çœŸåº¦: {np.mean(test_fidelities):.6f}")
    print(f"  æµ‹è¯•é›†æœ€ä½³ä¿çœŸåº¦: {max(test_fidelities):.6f}")


if __name__ == "__main__":
    main()
